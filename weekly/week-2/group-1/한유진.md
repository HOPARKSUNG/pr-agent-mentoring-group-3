# 개인 과제 (RAG, MCP, A2A)

## LLM 의 한계
- "이미 학습된 지식"만 가지고 답을 만든다.
- 최신 정보나 기업 내부 문서처럼 "학습되지 않은 데이터"는 모른다.
- 모르는 정보를 아는 척 대답하는 경우가 있음 => "Hanllucination(환각)"
- LLM의 한계를 극복하기 위해 생긴 기술 -> RAG, MCP, A2A

## RAG (Retrieval-Augmented Generation)
- Retrieval(검색) + 증강 + Generation (생성)
- 원리
  - LLM 모델이 바로 대답하는 것이 아닌, 먼저 "외부 지식 베이스"에 검색을 해서 필요한 정보를 가져온 다음, 검색정보 기반으로 답변을 생성
- 구성 요소
  - 검색기(Retriever) : 필요한 정보를 찾는 역할 - 예) 백터 검색
  - 생성기(Reader) : 찾은 정보를 바탕으로 문장을 짜는 역할
- 장점
  - 최신 정보나 기업 내부 문서 등을 쉽게 반영함
  - 환각 감소
- 단점
  - 검색된 정보에 따라 정보의 질이 달라짐
  - 검색 품질에 따라 답변의 질이 달라짐
- 아키텍처
  - 구성요소
    1. 사용자 질의 입력: 사용자가 질문을 입력
    2. 임베딩 생성: 질의를 벡터 형태로 변환
    3. 문서 검색: 벡터 데이터베이스에서 관련 문서를 검색
    4. 문서 임베팅: 검색된 문서를 벡터로 변환
    5. LLM입력: 질의와 관련 문서를 LLM에 입력함
    6. 응답 생성: LLM이 최종 응답을 생성
  - 실제 적용 사례
    - IBM: 고객 지원 챗봇에 RAG을 적용하여 신뢰할 수 있는 콘텐츠 기반의 응답을 제공
    - 관련 ) https://www.projectpro.io/article/rag-use-cases-and-applications/1059

## MCP(Model-Centric Prompting)
- 모델 중심 프롬프트 
- 원리
  - LLM 자체를 더 똑똑하게 프롬프트를 작성해서 오답/환각을 줄이는 방식
  - 외부 지식이나 검색 없이, "질문을 꼼꼼하고 똑똑하게" 하는 방식
- 방법
  - 질문을 할때 명확한 조건 제공 
    - 예) 참고 문헌이 없다면 대답하지마. 모르면 모른다고 해
  - 답변 포맷을 정해주기
    - 예) 항상 json 형식으로 대답해
  - 다단계 질문하기
    - 예) 1단계: 문제 분석 -> 2단계: 답변 생성
- 장점
  - 별도 외부 시스템 없이 LLM만 잘사용하면 된다.
  - 세밀한 제어가 가능함
- 단점
  - 근본적으로 LLM 내부 지식 한계에 대한 수정 X
  - 여전히 환각 가능성 존재
  - 프롬프트 설계가 어려움
- 아키텍처
  - 구성요소
    1. 프롬프트 설계: 모델의 응답을 유도하기 위한 정교한 프롬프트를 작성
    2. Few-Shot 학습: 모델에 몇 가지 예시를 제공하여 학습 효과를 높임
    3. 응답 생성: 모델이 프롬프트에 따라 응답을 생성
  - 사례
    1. 감정 분석 도구: 고객 피드백을 "긍정", "부정", "중립"으로 분류하는 데 Few-Shot 프롬프트를 활용
    2. 학술 연구 지원: 연구자가 키워드나 이전 작업을 기반으로 연구 방향을 제안받을 수 있도록 프롬프트를 설계

## A2A(Agent to Agent)
- Agent끼리의 협력
- 원리
  - 여러 LLM의 인스턴스끼리 서로 대화를 하게하며, 문제를 풀게 한다.
  - LLM 모델끼리 회의를 시켜, 더 나은 답을 내는 방식
- 대표적인 구조
  - Planner Agent: 문제를 쪼개고 계획을 세움
  - Executor Agent: 세부 작업을 실행
  - Reviewer Agent: 결과를 검토하고 수정
- 장점
  - 문제를 분업화해서 처리하기 때문에 복잡한 문제에 더 강함
  - 서로 체크하고 검토해서 환각이나 실수를 줄인다.
- 단점
  - 시스템이 복잡해진다.
  - 에이전트 간 대화 비용이 많아지면 응답이 느려진다.
  - 잘못 설계했을 경우 비효율적인 회의만 진행될 수 있다.
- 아키텍처
  - 구성요소
    1. 플래너 에이전트: 작업을 계획하고 하위 작업으로 분할
    2. 실행자 에이전트: 각 하위 작업을 수행
    3. 검토자 에이전트: 작업 결과를 검토하고 수정
    4. 에이전트 간 통신: 에이전트들이 상호작용하여 최종 결과를 도출
  - 사례
    - Netflix 및 Amazon: 학습 에이전트를 활용하여 사용자 추천 시스템을 개선
    - AMD의 Gaia 프로젝트: 로컬에서 실행되는 LLM 에이전트를 통해 사용자 쿼리를 향상시키고 오프라인 기능을 제공

## MCP (Model Context Protocol): 모델과 외부 도구 간의 연결
- 개요
  - MCP는 Anthropic에서 개발한 프로토콜로, AI 모델이 외부 도구나 데이터를 동적으로 활용할 수 있도록 지원합니다.​

- 구성 요소 
  - MCP 호스트: 사용자 인터페이스를 제공하며, 예를 들어 Claude Desktop이나 코딩 도우미와 같은 애플리케이션이 이에 해당
  - MCP 클라이언트: 호스트와 MCP 서버 간의 보안된 연결을 관리하며, 다양한 도구와 데이터 소스에 접근할 수 있도록 중개 역할 
  - MCP 서버: 특정 도구나 기능을 제공하는 서비스로, 예를 들어 파일 검색, 데이터 조회, 외부 API 호출 등을 수행
- 기능 
  - MCP를 통해 AI 모델은 실행 중에 필요한 도구나 데이터를 동적으로 호출할 수 있으며, 이를 통해 더 정확하고 풍부한 응답을 생성할 수 있습니다.

## A2A와 MCP의 상호 보완성
A2A와 MCP는 각각 에이전트 간의 수평적 통합과 모델과 도구 간의 수직적 통합을 담당하여, 함께 사용될 때 더욱 강력한 AI 에이전트 생태계를 구축할 수 있다.
- A2A: 에이전트들이 서로 협력하여 복잡한 작업을 분산 처리할 수 있도록 지원합니다. 
- MCP: AI 모델이 외부 도구나 데이터를 활용하여 더 나은 추론과 응답을 생성할 수 있도록 지원합니다.