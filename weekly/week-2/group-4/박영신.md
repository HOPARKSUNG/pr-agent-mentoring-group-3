## RAG 정리

**🔗 참고 자료**

- NVIDIA 블로그: ["What is Retrieval-Augmented Generation?"](https://blogs.nvidia.com/)
- StackOverflow 블로그: ["Practical Tips for Retrieval-Augmented Generation"](https://stackoverflow.blog/)
- AWS 블로그: ["RAG란 무엇인가"](https://aws.amazon.com/ko/blogs/)
- REALM 논문: [REALM: Retrieval-Augmented Language Model Pre-Training (Guu et al., 2020)](https://arxiv.org/abs/2002.08909)

### **1. RAG란 무엇인가?**

**Retrieval-Augmented Generation (RAG)** 은 **외부 지식 검색** + **텍스트 생성**을 결합한 방법으로, LLM이 **자신의 파라미터에 저장된 지식**만 사용하는 대신, **외부 데이터 소스(문서, DB, 위키 등)** 에서 **관련 정보를 검색**해 그 정보를 **컨텍스트로 활용**하여 답변을 생성하는 것이다.

**비유**

기존 LLM이 "닫힌 책 시험"을 본다면, RAG를 쓴 LLM은 "열린 책 시험"을 본다.

> "질문 → 검색 → 답변 생성" 흐름을 자동화하는 시스템
> 

---

### 2.  RAG가 필요한 이유

| **문제** | **설명** | **RAG가 해결하는 방법** |
| --- | --- | --- |
| 모델 지식 최신성 부족 | 학습 이후 생긴 정보는 알지 못함 | 질문 시점에 실시간 검색 |
| 환각(Hallucination) 문제 | 그럴듯하지만 틀린 답변을 생성 | 외부 근거를 근거로 답변 생성 |
| 재학습 비용 | 새로운 지식을 반영하려면 모델 전체 파인튜닝 필요 | 지식베이스만 업데이트 |
| 대형 모델 부담 | 모든 지식을 모델에 넣으려면 초거대 모델 필요 | 소형 모델 + 외부 지식 연결 |
| 답변 투명성 부족 | 어디서 정보를 가져왔는지 알 수 없음 | 출처와 함께 답변 가능 |

---

### 3. RAG의 작동 원리

1. **지식베이스 구축**
    - 문서를 작은 청크(chunk)로 분할
    - 각 청크를 임베딩(벡터화)
    - 벡터 데이터베이스(Vector DB)에 저장
2. **질문 임베딩 및 검색**
    - 사용자의 질문을 벡터로 변환
    - Vector DB에서 **가장 유사한 문서 청크**들을 검색
3. **프롬프트 보강**
    - 검색된 문서들을 **컨텍스트로 추가**하여 LLM에 입력
4. **LLM 답변 생성**
    - 질문 + 검색된 정보 기반으로 답변 생성
    - 필요 시 출처(citation) 포함

---

### 4. RAG 아키텍처 구성 요소

| **컴포넌트** | **역할** |
| --- | --- |
| **임베딩 모델** | 텍스트를 고차원 벡터로 변환 (문서/질문 둘 다) |
| **벡터 데이터베이스 (Vector DB)** | 임베딩된 문서 청크를 저장하고, 유사 검색 |
| **검색기 (Retriever)** | 질문 임베딩을 기반으로 관련 문서 검색 |
| **생성 모델 (Generator, LLM)** | 질문 + 검색 문서를 바탕으로 최종 답변 생성 |

**기술 예시**

- 벡터 DB: Faiss, Pinecone, Weaviate, Qdrant
- 임베딩 모델: OpenAI ada-002, Sentence-BERT, Cohere
- 생성 모델: GPT-3.5, GPT-4, Llama 2

---

### 5. RAG의 장점

**[1] 사실성 향상 및 환각 감소**

- 기존 LLM은 모르는 것도 그럴듯하게 답변하는 "환각"이 많음
    
    → LLM은 훈련할 때 배운 정보만 기억하고, 그 이후 생긴 건 모르는 상태로 그냥 추측해야 하기 때문
    
- **RAG**는 질문할 때 **외부에서 관련 자료를 찾아**와서 답변하기 때문에, 추측하는 대신, **근거를 기반으로 정확한 답변 가능**

**[2] 지식 최신성 보장**

- LLM은 학습한 시점 이후의 사건(예: 2024년 뉴스) 같은 걸 기본적으로 모르지만, RAG를 사용하면, 질문 시점에 바로 **최신 문서**를 검색해서 **새로운 정보**를 답변에 반영 가능

**[3] 모델 경량화**

- 원래는 지식이 많을수록 **모델 크기(파라미터 수)** 를 키워야 했음 → 거대 LLM
- 하지만 RAG를 쓰면, 모델 자체는 가볍게 유지하고 필요할 때 외부 지식베이스를 조회하면 되기에 모델이 가벼워짐

**[4] 도메인 특화 적응**

- LLM이 기본적으로는 범용 지식만 가지고 있으나 RAG는 특정 산업(예: 의료, 법률, 금융), 특정 회사(예: 삼성전자 내부 규정) 등 이런 전문적/내부적 지식까지 붙여주는 것이 가능함

---

### 6. RAG의 한계와 주의점

| **문제점** | **설명** |
| --- | --- |
| 검색 실패 위험 | 잘못된 문서를 검색하면 틀린 답변 생성 가능 |
| 컨텍스트 길이 한계 | LLM 프롬프트 길이에 제한 (수천 토큰) |
| 구축 복잡성 | 벡터 DB, 검색 파이프라인 등 추가 필요 |
| 데이터 품질 중요 | 쓰레기 데이터 → 쓰레기 답변 |
| 보안 이슈 | 민감 정보 벡터화/검색 과정에서 유출 위험 |
| 프롬프트 설계 필요 | 컨텍스트를 효과적으로 활용하도록 최적화해야 함 |

<br>
<br>

---

## MCP 정리

**🔗 참고 자료**

- [YouTube 영상: MCP에 대한 짧은 설명 (2024)](https://www.youtube.com/watch?v=KZwV6dOfNuE)
- [Pytorch Discuss: MCP에 대한 글 정리](https://discuss.pytorch.kr/t/deep-research-model-context-protocol-mcp/6594/1)
- [Velog: What is MCP?](https://velog.io/@k-svelte-master/what-is-mcp?utm_source=oneoneone)

### 1. MCP란 무엇인가?

MCP(Model Context Protocol)는 모델이 이전 대화(Context)를 **효율적**으로 **이해하고 기억**하도록 돕기 위한 **프로토콜**이다.

구글 딥마인드(DeepMind) 연구진이 제안했다.

한마디로 요약하면, **"메모리 효율적으로 대화 문맥을 복원해주는 시스템"** 이다.

기존 LLM은 긴 대화를 하려면 **모든 이전 대화를 통째로 다시 입력**해야 했지만 이건 토큰 비용이 너무 크고 비효율적이었다.

MCP는 이 문제를 해결하고자 등장한 것이다.

---

### 2. 왜 MCP가 필요한가?

**(1) 기존 문제점**

- LLM은 이전 대화를 기억하려면 모든 히스토리(대화 기록)를 다시 입력해야 한다.
- 입력 토큰이 너무 커지면
    - 비용(=비용 폭발)
    - 속도 저하
    - 모델 성능 악화
    등의 문제가 생긴다.

**(2) 그래서 MCP 등장하게 되었다.**

- 대화 히스토리를 전부 넣는 대신, **"요약된 압축 정보"** 만 모델에게 제공하자는 것이다.

---

### 3. MCP의 구조와 핵심 개념

MCP는 크게 두 가지 역할을 수행한다.

| **역할** | **설명** |
| --- | --- |
| **Encoding (Context Embedding)** | 과거 대화 내용을 **벡터**로 요약하여 저장 |
| **Retrieval + Injection** | 새로운 질문이 들어오면, 과거 대화 중 **관련 있는 벡터**만 골라서 모델에게 제공 |

이때 사용하는 두 개의 핵심 컴포넌트는 다음과 같다.

- **Memory Encoder**
    - 과거 대화의 요약 정보를 만든다.
    - 문맥의 핵심을 잡아 **Context Embedding**을 생성한다.
- **Memory Retriever**
    - 현재 질문에 맞는 과거 문맥을 찾아온다.
    - 찾아온 요약(Context Embedding)을 현재 질문과 함께 모델에 제공한다.

---

### 4. MCP의 동작 방식

MCP의 대략적인 처리 순서는 다음과 같다.

1. 사용자가 LLM에게 질문을 한다.
2. LLM이 응답을 생성한 후, 대화 기록을 **Memory Encoder**가 요약(=context embedding 생성)한다.
3. 생성된 embedding은 **메모리 공간**에 저장된다.
4. 새로운 질문이 들어오면, 아래의 과정을 진행한다.
    - **Memory Retriever**가 현재 질문을 분석
    - 과거 저장된 embedding 중에서 **관련 있는 것들만** 선택
5. 선택된 embedding과 현재 질문을 합쳐서 LLM에 다시 제공한다.

→ 즉, 모든 과거 대화를 다 넣지 않고, **정제된 요약**만 사용해서 문맥을 이어가는 방식이다.

```css
[과거 대화] → [Memory Encoder] → [압축된 Context Embedding 저장]
[새 질문] → [Memory Retriever] → [관련 Embedding 조회] → [LLM에 제공]
```

---

### 5. 기존 방법과 MCP의 차이점

| **비교 항목** | **기존 방식** | **MCP 방식** |
| --- | --- | --- |
| 문맥 처리 | 과거 대화 모두 재입력 | 요약 embedding만 재활용 |
| 비용 | 토큰 수 많아 비용 폭발 | 토큰 절약 |
| 응답 속도 | 느려짐 | 빨라짐 |
| 문맥 유지력 | 긴 대화 시 오류 가능성 증가 | 핵심 문맥만 유지 |

→ MCP는 "필요한 기억만 뽑아와서" 문맥을 유지하는 게 핵심 !

---

### 6. MCP의 한계와 앞으로의 방향

**(1) 현재의 한계점**

- Memory Encoder의 품질이 좋지 않으면, 문맥 요약이 부정확해서 대화 흐름이 깨질 수 있다.
    
- 어떤 과거 문맥을 가져와야 할지(=retrieval) 정확히 판단하는 게 여전히 어렵다.

**(2) 발전 방향**

- 더 좋은 Encoder 모델 개발 (더 똑똑하게 요약)
- Retriever의 정확도 향상
- 여러 Memory layer(장기 기억, 단기 기억 등)로 확장

---

### 요약

- MCP는 **대화 기록을 압축 저장하고**, **필요할 때 요약본만 가져와 쓰는** 프로토콜
- 토큰 비용 줄이고, 긴 대화에서도 성능을 유지하는 방법
- Encoder와 Retriever 품질이 MCP 성능을 좌우함

---