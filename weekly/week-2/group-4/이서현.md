# RAG

### 1. 개요
검색-증강 생성(RAG, Retrieval-Augmented Generation)은 대규모 언어 모델(LLM)의 한계를 보완하기 위한 기술로, 모델이 응답을 생성하기 전 외부 지식 베이스에서 관련 정보를 검색하여 프롬프트에 포함시킨 후 응답을 생성하는 방식이다. 이를 통해 모델 재학습 없이도 정확하고 최신의 정보를 제공할 수 있다.

### 2. 필요성
LLM은 고정된 학습 데이터를 기반으로 작동하기 때문에 최신 정보 반영이 어렵고, 환각(hallucination) 문제로 인해 신뢰할 수 없는 응답을 생성할 수 있다. 또한 동일한 용어가 다양한 문맥에서 혼용될 수 있어 부정확한 결과를 초래할 수 있다. RAG는 이러한 문제를 해결하기 위한 현실적인 대안이다.

### 3. 주요 장점
- 비용 효율성: 도메인 특화 데이터를 반영하기 위해 별도의 모델 재학습이 불필요하다.
- 정보 최신성 확보: 뉴스, 실시간 데이터 등 자주 갱신되는 소스를 연동해 최신 정보를 제공할 수 있다.
- 사용자 신뢰성 제고: 출처 기반 응답이 가능해 인용이나 참조가 포함된 응답을 제공할 수 있다.
- 개발 유연성 강화: 외부 지식 소스를 관리·제어함으로써 특정 부서나 인증 수준에 따라 정보 접근을 조절할 수 있다.

### 4. 작동 방식
RAG는 다음의 절차로 구성된다.
1) 외부 데이터 확보: 문서, DB, API 등에서 비정형 데이터를 수집하고 임베딩 모델을 통해 벡터화한다.
2) 정보 검색: 사용자의 질의를 벡터 표현으로 변환하여 관련도 높은 문서를 벡터 DB에서 검색한다.
3) 프롬프트 확장: 검색된 문서를 기반으로 LLM 입력 프롬프트를 보강한다.
4) 응답 생성: LLM이 확장된 컨텍스트를 활용해 최종 응답을 생성한다.
5) 데이터 최신화: 외부 문서의 변경에 따라 벡터 표현도 주기적으로 갱신한다.

### 5. 시맨틱 검색과의 차이점
시맨틱 검색은 문서 간 의미 유사도 기반 검색을 수행하는 기술이며, RAG는 이러한 검색 결과를 프롬프트에 통합해 언어 모델의 생성 품질을 향상시키는 구조를 갖는다. 시맨틱 검색은 대규모 지식 탐색에 적합하며, RAG는 이를 생성형 AI 응답에 적용하여 실질적인 활용도를 높인다.

### 6. 클라우드와 함께 활용하는 사례
AWS는 Amazon Bedrock을 통해 벡터화, 검색, 응답 생성을 자동화한 RAG 워크플로를 제공하며, Amazon Kendra는 고정밀 시맨틱 검색 기능을 통해 RAG의 검색 엔진으로 활용된다. 이외에도 SageMaker JumpStart를 통해 사용자 정의 생성 AI 솔루션 구현을 지원한다.

### 7. 참고자료
https://aws.amazon.com/ko/what-is/retrieval-augmented-generation/

---
<br>
<br>

# MCP

### 1. 개요
모델 컨텍스트 프로토콜(MCP, Model Context Protocol)은 애플리케이션이 대규모 언어 모델(LLM)에 컨텍스트를 전달하는 방식을 표준화한 개방형 프로토콜이다. 이는 다양한 데이터 소스 및 툴과 LLM을 일관된 방식으로 연결할 수 있도록 설계되었으며, AI 개발 환경에서 범용 포맷으로 기능한다.

### 2. 필요성
LLM은 실제 서비스에 적용되기 위해 다양한 데이터와 도구들과의 통합이 필수적이다. MCP는 이러한 통합 과정을 단순화하고, 데이터 보안 및 유지관리의 효율성을 확보하며, 다양한 LLM 제공업체 간 전환을 유연하게 할 수 있도록 지원한다.

### 3. 주요 장점
- 통합성: 다양한 MCP 서버(데이터 소스 또는 도구)를 하나의 클라이언트에서 통합적으로 접근 가능
- 유연성: Claude, IDE 등 다양한 LLM 기반 클라이언트에서 재사용 가능
- 보안성: 로컬 데이터와 외부 API 모두 안전하게 연동 가능
- 모듈화: 서버 단위로 기능 분리되어 유지보수와 확장이 용이

### 4. 작동 방식
MCP는 클라이언트-서버 구조를 기반으로 한다. 각 클라이언트는 다수의 MCP 서버와 연결되어 파일, DB, API 등의 데이터를 LLM에 전달한다.
- MCP Host: LLM 기반 애플리케이션 (예: Claude, IDE 등)
- MCP Client: Host에서 실행되는 프로토콜 클라이언트
- MCP Server: 로컬 또는 원격 데이터를 MCP 형식으로 제공하는 모듈

### 5. 특징 및 비교
- 범용성: 특정 LLM 또는 플랫폼에 종속되지 않음
- 확장성: 프롬프트 템플릿, 액션 수행, LLM 샘플링 요청 등 고급 기능 지원
- 기존 개별 API 기반 연결 대비 개발 난이도 및 유지보수 비용을 크게 절감할 수 있음

### 6. 관련 서비스 사례
- Claude for Desktop: MCP 기반으로 GitHub 등 다양한 외부 시스템과의 통합 작업을 단시간 내 구현 가능
- MCP SDK 및 서버 예시: 개발자는 공식 MCP 서버 템플릿 및 클라이언트 구현 예제를 활용해 독자적인 에이전트 환경을 구축할 수 있음

### 7. 참고자료
https://docs.anthropic.com/ko/docs/agents-and-tools/mcp

---
<br>
<br>

# A2A

### 1. 개요
Agent2Agent Protocol(A2A)은 서로 다른 프레임워크 또는 벤더에서 개발된 독립적인 AI 에이전트 간의 상호운용성과 통신을 지원하는 개방형 프로토콜이다. Google이 주도하는 본 프로토콜은 에이전트 간 협업을 위한 공통 언어를 정의하며, 텍스트, 양식, 음성/영상 등 다양한 방식으로 사용자 상호작용을 조정할 수 있게 한다.

### 2. 필요성
기업 환경에서 다양한 LLM 기반 에이전트들이 독립적으로 개발되고 있음에도 불구하고, 이들 간의 연동이 어렵다는 점이 AI 도입의 핵심 장애 요소 중 하나로 지적된다. A2A는 이러한 이기종 에이전트 간 통합 문제를 해결하기 위해 설계되었다.

### 3. 주요 장점
- 표준화된 통신 구조: 모든 에이전트가 동일한 방식으로 자신의 기능을 외부에 공개하고 요청을 수신
- 멀티 벤더 호환성: 프레임워크·제공사 무관하게 연결 가능
- 확장 가능 구조: 상태 업데이트, 양방향 메시징, 실시간 스트리밍 등 다양한 기능 포함
- 보안 지원: 인증 요구사항 및 접근 제어 메커니즘을 Agent Card 수준에서 명시 가능

### 4. 작동 방식
A2A는 클라이언트-서버 모델에 기반하여 다음과 같은 흐름으로 작동한다.
- 발견(Discovery): 클라이언트가 서버의 /.well-known/agent.json에서 Agent Card를 가져와 에이전트를 식별
- 작업 생성(Task): 클라이언트가 tasks/send 또는 tasks/sendSubscribe를 통해 작업 요청을 보냄
- 처리 및 스트리밍: 서버는 SSE(Server-Sent Events)를 통해 작업 상태나 결과를 실시간 전달
- 양방향 상호작용: 입력 필요 시 클라이언트가 동일 Task ID로 추가 메시지 전송
- 완료 처리: 작업은 완료, 실패, 취소 중 하나의 상태로 종료됨
- Push 알림: 웹훅 URL 등록을 통해 비동기 상태 알림도 지원 가능

### 5. 특징 및 비교
- Agent Card 기반 자동 인식 구조: 각 에이전트가 JSON 기반 메타데이터를 통해 기능을 선언
- Streaming 및 Push Notification 병행 지원: 장기 작업에 적합한 구조 제공
- MCP와의 차별점: MCP는 LLM과의 단일 컨텍스트 통신 표준화에 초점을 두고, A2A는 다중 에이전트 간 협업 프로토콜에 중점을 둠
- 작업 중심(Task-Centric) 구조: 모든 상호작용은 Task ID를 기반으로 진행되어 상태 추적이 용이

### 6. 관련 서비스 및 사례
- 샘플 클라이언트/서버: Python, JS 기반 예제 제공
- 지원 프레임워크: CrewAI, LangGraph, Genkit, LlamaIndex, Semantic Kernel 등에서 A2A 연동 가능
- 개발 도구: ADK(Agent Development Kit), A2A Inspector, JSON Schema 기반 구조화 문서 제공
- 향후 계획: 인증 스키마 강화, QuerySkill 메소드 도입, 스트리밍 및 UX 협상 기능 개선 예정

### 7. 참고자료
https://github.com/google/A2A