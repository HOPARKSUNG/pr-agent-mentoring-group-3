# RAG

### 1. 개요
검색-증강 생성(RAG, Retrieval-Augmented Generation)은 대규모 언어 모델(LLM)의 한계를 보완하기 위한 기술로, 모델이 응답을 생성하기 전 외부 지식 베이스에서 관련 정보를 검색하여 프롬프트에 포함시킨 후 응답을 생성하는 방식이다. 이를 통해 기존 LLM이 갖는 정보 고정성, 환각 문제, 도메인 불일치 등의 한계를 보완하고, 최신성과 정확성이 확보된 응답을 생성할 수 있다.

### 2. 필요성
기존 LLM은 사전 학습된 정적 파라미터에 의존해 응답을 생성한다. 이에 따라 다음과 같은 문제가 발생한다.
- 최신 정보 반영 불가
- 불확실한 질문에 대한 허위 응답 생성
- 도메인별 정확도 저하 및 출처 불명
- 데이터 갱신을 위한 재학습 비용 과다
RAG는 이러한 문제를 사후 처리 없이 해결할 수 있는 동적 문서 기반 지식 주입 방식으로, 산업 및 엔터프라이즈 환경에서 필수적인 보완 기술로 주목받고 있다.

### 3. 주요 장점
- 비용 효율성: 파운데이션 모델을 유지하면서 데이터만 갱신 가능
- 확장성: 다양한 도메인 지식과 실시간 정보의 통합 용이
- 투명성 및 신뢰성: 인용 기반 응답 가능 → 사용자가 결과 근거를 검증 가능
- 모듈성: 검색 시스템, 벡터 DB, 생성 모델을 독립적으로 개선 가능

### 4. 작동 원리 및 시스템 구성
#### 4.1 전체 파이프라인 구조
사용자 질의 → 질의 임베딩 → 벡터 검색 → 관련 문서 추출 → 프롬프트 생성 → LLM 응답

#### 4.2 구성 요소 및 구현 방법
구성 요소	설명	구현 기술 예시
문서 임베딩	텍스트 문서를 벡터로 변환	SentenceTransformers, OpenAI Embedding, HuggingFace
벡터 저장소	벡터 인덱싱 및 검색	FAISS, Pinecone, Weaviate, Qdrant
질의 처리	사용자 입력을 벡터로 변환 후 최근접 검색 수행	cosine similarity, dot product
프롬프트 구성기	검색된 문서를 LLM 입력에 맞춰 문맥으로 조합	템플릿 엔지니어링, Chunking, Reranking
언어 모델	문맥 기반 응답 생성	OpenAI GPT, Claude, Mistral, LLaMA

#### 4.3 세부 흐름
1. 문서 전처리 및 임베딩 저장
    - 문서를 일정 길이로 청킹(chunking)
    - 각 문서를 벡터화 후 벡터 DB에 저장
2. 질의 벡터화 및 유사도 검색
    - 사용자의 질의를 동일한 임베딩 모델로 벡터화
    - 벡터 DB에서 유사 문서 K개 추출
3. 프롬프트 확장
    - 원 질의 + 검색된 문서를 포함한 형태로 LLM 입력 구성
    - 예: “문제: X\n참고 자료:\n- 문서1\n- 문서2…”
4. 응답 생성 및 후처리
    - LLM이 확장된 프롬프트에 기반하여 응답 생성
    - 출처를 명시하거나 결과에 포함시킬 수 있음

### 5. 시맨틱 검색과의 차이점
시맨틱 검색은 문서 간 의미 유사도 기반 검색을 수행하는 기술이며, RAG는 이러한 검색 결과를 프롬프트에 통합해 언어 모델의 생성 품질을 향상시키는 구조를 갖는다. 시맨틱 검색은 대규모 지식 탐색에 적합하며, RAG는 이를 생성형 AI 응답에 적용하여 실질적인 활용도를 높인다.

### 6. 클라우드와 함께 활용하는 사례
- Amazon Bedrock: RAG 기반 워크플로를 손쉽게 구현할 수 있는 Fully Managed AI 플랫폼
- Amazon Kendra: 의미 기반 벡터 검색을 지원하는 시맨틱 검색 엔진
- SageMaker JumpStart: 사전 구축된 생성 AI 파이프라인 템플릿을 통해 RAG를 빠르게 실험 가능

### 7. 참고자료
- AWS RAG 소개: https://aws.amazon.com/ko/what-is/retrieval-augmented-generation/
- Hugging Face RAG 소개: https://huggingface.co/docs/transformers/model_doc/rag


---
<br>
<br>

# MCP

### 1. 개요
모델 컨텍스트 프로토콜(MCP, Model Context Protocol)은 애플리케이션이 대규모 언어 모델(LLM)에 컨텍스트를 전달하는 방식을 표준화한 개방형 프로토콜이다. 이는 다양한 데이터 소스 및 툴과 LLM을 일관된 방식으로 연결할 수 있도록 설계되었으며, AI 개발 환경에서 범용 포맷으로 기능한다.

### 2. 필요성
LLM은 실제 서비스에 적용되기 위해 다양한 데이터와 도구들과의 통합이 필수적이다. MCP는 이러한 통합 과정을 단순화하고, 데이터 보안 및 유지관리의 효율성을 확보하며, 다양한 LLM 제공업체 간 전환을 유연하게 할 수 있도록 지원한다.

### 3. 주요 장점
- 통합성: 다양한 MCP 서버(데이터 소스 또는 도구)를 하나의 클라이언트에서 통합적으로 접근 가능
- 유연성: Claude, IDE 등 다양한 LLM 기반 클라이언트에서 재사용 가능
- 보안성: 로컬 데이터와 외부 API 모두 안전하게 연동 가능
- 모듈화: 서버 단위로 기능 분리되어 유지보수와 확장이 용이

### 4. 작동 방식
MCP는 클라이언트-서버 구조를 기반으로 한다. 각 클라이언트는 다수의 MCP 서버와 연결되어 파일, DB, API 등의 데이터를 LLM에 전달한다.
- MCP Host: LLM 기반 애플리케이션 (예: Claude, IDE 등)
- MCP Client: Host에서 실행되는 프로토콜 클라이언트
- MCP Server: 로컬 또는 원격 데이터를 MCP 형식으로 제공하는 모듈

### 5. 특징 및 비교
- 범용성: 특정 LLM 또는 플랫폼에 종속되지 않음
- 확장성: 프롬프트 템플릿, 액션 수행, LLM 샘플링 요청 등 고급 기능 지원
- 기존 개별 API 기반 연결 대비 개발 난이도 및 유지보수 비용을 크게 절감할 수 있음

### 6. 관련 서비스 사례
- Claude for Desktop: MCP 기반으로 GitHub 등 다양한 외부 시스템과의 통합 작업을 단시간 내 구현 가능
- MCP SDK 및 서버 예시: 개발자는 공식 MCP 서버 템플릿 및 클라이언트 구현 예제를 활용해 독자적인 에이전트 환경을 구축할 수 있음

### 7. 참고자료
- Anthropic MCP 문서: https://docs.anthropic.com/ko/docs/agents-and-tools/mcp

---
<br>
<br>

# A2A

### 1. 개요
Agent2Agent Protocol(A2A)은 서로 다른 프레임워크 또는 벤더에서 개발된 독립적인 AI 에이전트 간의 상호운용성과 통신을 지원하는 개방형 프로토콜이다. Google이 주도하는 본 프로토콜은 에이전트 간 협업을 위한 공통 언어를 정의하며, 텍스트, 양식, 음성/영상 등 다양한 방식으로 사용자 상호작용을 조정할 수 있게 한다.

### 2. 필요성
기업 환경에서 다양한 LLM 기반 에이전트들이 독립적으로 개발되고 있음에도 불구하고, 이들 간의 연동이 어렵다는 점이 AI 도입의 핵심 장애 요소 중 하나로 지적된다. A2A는 이러한 이기종 에이전트 간 통합 문제를 해결하기 위해 설계되었다.

### 3. 주요 장점
- 표준화된 통신 구조: 모든 에이전트가 동일한 방식으로 자신의 기능을 외부에 공개하고 요청을 수신
- 멀티 벤더 호환성: 프레임워크·제공사 무관하게 연결 가능
- 확장 가능 구조: 상태 업데이트, 양방향 메시징, 실시간 스트리밍 등 다양한 기능 포함
- 보안 지원: 인증 요구사항 및 접근 제어 메커니즘을 Agent Card 수준에서 명시 가능

### 4. 작동 방식
A2A는 클라이언트-서버 모델에 기반하여 다음과 같은 흐름으로 작동한다.
- 발견(Discovery): 클라이언트가 서버의 /.well-known/agent.json에서 Agent Card를 가져와 에이전트를 식별
- 작업 생성(Task): 클라이언트가 tasks/send 또는 tasks/sendSubscribe를 통해 작업 요청을 보냄
- 처리 및 스트리밍: 서버는 SSE(Server-Sent Events)를 통해 작업 상태나 결과를 실시간 전달
- 양방향 상호작용: 입력 필요 시 클라이언트가 동일 Task ID로 추가 메시지 전송
- 완료 처리: 작업은 완료, 실패, 취소 중 하나의 상태로 종료됨
- Push 알림: 웹훅 URL 등록을 통해 비동기 상태 알림도 지원 가능

### 5. 특징 및 비교
- Agent Card 기반 자동 인식 구조: 각 에이전트가 JSON 기반 메타데이터를 통해 기능을 선언
- Streaming 및 Push Notification 병행 지원: 장기 작업에 적합한 구조 제공
- MCP와의 차별점: MCP는 LLM과의 단일 컨텍스트 통신 표준화에 초점을 두고, A2A는 다중 에이전트 간 협업 프로토콜에 중점을 둠
- 작업 중심(Task-Centric) 구조: 모든 상호작용은 Task ID를 기반으로 진행되어 상태 추적이 용이

### 6. 관련 서비스 및 사례
- 샘플 클라이언트/서버: Python, JS 기반 예제 제공
- 지원 프레임워크: CrewAI, LangGraph, Genkit, LlamaIndex, Semantic Kernel 등에서 A2A 연동 가능
- 개발 도구: ADK(Agent Development Kit), A2A Inspector, JSON Schema 기반 구조화 문서 제공
- 향후 계획: 인증 스키마 강화, QuerySkill 메소드 도입, 스트리밍 및 UX 협상 기능 개선 예정

### 7. 참고자료
- Google A2A 문서: https://github.com/google/A2A