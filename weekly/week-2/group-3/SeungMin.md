
# Week 2 보고서

## PR Agent 연계 개념 (RAG · MCP · A2A) 조사 보고서

>   작성자: 이승민 | 기간: 2025-04-21 ~ 04-27
>   참고 : [Week 2 Notion 자료](https://www.notion.so/akileo/Assignment-Week2-What-is-PR-Agent-1dcab55c647080e781dcde70582732b0)

---

## 1. PR 코드 리뷰 자동화의 필요성

소프트웨어 개발 과정에서 Pull Request (PR)는 코드 변경 사항을 공유하고 검토하는 중요한 절차이다.  그러나 PR의 양이 증가함에 따라 코드 리뷰 과정에서 다음과 같은 문제점이 발생할 수 있다.

| 문제점         | 설명                                                                                                                                                                                            |
| :------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **리뷰 지연** | PR이 과도하게 많아지면 리뷰 담당자의 업무 부담이 가중되고, 이는 리뷰 지연으로 이어져 최종 배포까지의 시간을 지연시키는 주요 요인이 된다.                                                              |
| **품질 편차** | 리뷰어의 경험 수준이나 당일 컨디션에 따라 리뷰 품질에 편차가 발생할 수 있다. 이는 코드베이스의 일관성을 저해하고 잠재적인 버그를 놓칠 가능성을 증가시킨다.                                       |
| **정적 분석 한계** | Linter와 같은 정적 분석 도구는 기본적인 문법 오류는 탐지할 수 있으나, 코드의 논리적 결함이나 설계상의 문제, 보안 취약점 등 **의미적인** 오류를 식별하는 데에는 한계가 있다.                                |

이러한 문제점들을 해결하기 위해 LLM (Large Language Model)을 활용한 자동화된 PR 코드 리뷰 시스템이 주목받고 있다. LLM은 PR diff를 분석하고 코드의 변경 사항을 요약하거나 잠재적인 문제점을 지적함으로써 리뷰 과정의 효율성과 일관성을 향상시킬 수 있다.

---

## 2. PR Agent (Qodo Merge) 개요

~~~mermaid
flowchart LR
    A[Pull Request diff] --> B(Diff Parser)
    B --> C(Prompt Composer)
    C --> D(LLM Invoker)
    D --> E(Comment Formatter)
    E --> F[PR 코멘트 등록]
~~~

PR Agent (현재 Qodo Merge로 리브랜딩)는 LLM을 기반으로 PR 코드 리뷰를 자동화하는 오픈 소스 도구이다.  위 다이어그램은 PR Agent의 기본적인 작동 흐름을 나타낸다. 각 단계는 다음과 같은 역할을 수행한다.

-   **Diff Parser**: PR diff를 입력으로 받아 변경된 코드의 내용을 분석한다.
-   **Prompt Composer**: 분석된 코드 변경 사항을 LLM이 이해할 수 있는 자연어 프롬프트로 변환한다.
-   **LLM Invoker**: 생성된 프롬프트를 LLM에 전달하고 코드 리뷰 결과를 생성한다.
-   **Comment Formatter**: LLM이 생성한 리뷰 결과를 PR에 등록하기 적합한 형식으로 변환한다.
-   **PR 코멘트 등록**: 최종 리뷰 결과를 PR에 코멘트로 등록하여 개발자에게 피드백을 제공한다.

PR Agent는 다음과 같은 주요 기능을 제공한다.

| 항목         | 내용                                                                                                                                                                                                                                |
| :----------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 주요 명령    | `/describe` (PR 변경 사항 요약), `/review` (코드 리뷰 코멘트), `/test` (테스트 관련 코멘트) 등을 사용할 수 있다.                                                                                                                                         |
| 작동 방식    | 1.  Diff 분석: PR diff를 파싱하여 변경된 코드를 추출한다.<br>2.  프롬프트 생성: 추출된 코드를 LLM 입력에 적합한 형태로 변환한다.<br>3.  LLM 호출: LLM을 통해 코드 리뷰를 수행한다.<br>4.  결과 등록: 리뷰 결과를 PR 코멘트로 자동 등록한다.                               |
| 지원 모델    | GPT-4, Claude, 로컬 LLM (Ollama) 등 다양한 LLM을 지원한다.                                                                                                                                                                                  |
| 설치 방식    | GitHub App, GitHub Action, 온프레미스 Docker 등 다양한 환경에 설치할 수 있다.                                                                                                                                                               |
| 라이선스     | Apache-2.0 (오픈 소스)                                                                                                                                                                                                              |

---

## 3. GitHub Copilot Review와 비교

PR Agent 외에도 GitHub Copilot Review와 같은 유사한 코드 리뷰 자동화 도구가 존재한다.  두 도구의 주요 차이점은 다음과 같다.

| 항목             | **PR Agent** | **GitHub Copilot Review** |
| :--------------- | :--------------------------------- | :----------------- |
| 설치 위치        | 온프레미스 환경을 포함한 다양한 환경을 지원한다.       | GitHub SaaS 환경에서만 사용 가능하다.   |
| 모델 교체       | 가능하다.                               | 불가능하다.             |
| 프롬프트 수정    | YAML 기반으로 프롬프트를 자유롭게 튜닝할 수 있다. | 불가능하다.             |
| 비용 구조        | 오픈 소스(무료) + API 사용료가 발생할 수 있다.            | 유료 구독 모델이다.          |
| 사용자 정의       | 높은 수준의 사용자 정의가 가능하다.                               | 제한적인 사용자 정의만 가능하다.             |
| 통합 용이성      | 다양한 도구 및 환경과의 통합이 용이하다.                               | GitHub 환경에 특화되어 있다.      |

---

## 4. RAG (Retrieval-Augmented Generation) 상세 분석

~~~mermaid
sequenceDiagram
    participant Q as 질문
    Q->>V: 임베딩 변환 과정을 거친다.
    V-->>Q: 변환된 임베딩을 바탕으로 관련 문서 Top-k를 반환한다.
    Q->>L: 질문과 관련 문서를 함께 LLM에 입력한다.
    L-->>U: LLM은 주어진 정보를 바탕으로 근거를 포함한 답변을 생성한다.
~~~

RAG (Retrieval-Augmented Generation)는 LLM이 외부 지식 소스를 활용하여 답변을 생성하는 기술이다.  위 시퀀스 다이어그램은 RAG의 작동 방식을 보여준다.

-   **핵심**: LLM이 자체적으로 답변하기 어려운 질문에 대해 외부 검색 결과를 참조하여 답변의 정확도를 높이는 데 있다.  LLM은 방대한 양의 텍스트 데이터를 학습했지만, 모든 지식을 내재화하고 있지는 않다.  따라서 RAG는 LLM이 필요한 정보를 외부에서 검색하여 답변 생성에 활용함으로써, 답변의 정확성과 신뢰성을 향상시킨다.

-   **작동 원리**:
    1.  **질문 임베딩**: 주어진 질문을 벡터 형태로 변환한다.  임베딩은 텍스트 데이터를 수치화하여 LLM이 이해하고 처리하기 용이하게 만드는 과정이다.
    2.  **관련 문서 검색**: 변환된 질문 임베딩을 사용하여 외부 문서 데이터베이스에서 관련성이 높은 문서를 검색한다.  이때, 코사인 유사도(cosine similarity)와 같은 지표를 사용하여 질문과 문서 간의 의미적 유사도를 측정한다.
    3.  **프롬프트 구성**: 검색된 문서를 질문과 함께 LLM에 입력할 프롬프트로 구성한다.  프롬프트는 LLM에게 어떤 방식으로 답변해야 하는지에 대한 지침을 제공하는 역할을 한다.
    4.  **답변 생성**: LLM은 주어진 질문과 관련 문서를 바탕으로 답변을 생성한다.  이때, LLM은 검색된 문서에서 추출한 정보를 활용하여 답변의 근거를 제시하고, 환각(hallucination) 현상을 감소시킨다.

-   **장점**:
    * **최신 정보 반영**: RAG는 LLM이 학습 데이터에 포함되지 않은 최신 정보를 답변에 반영할 수 있도록 한다.  외부 데이터베이스를 통해 실시간으로 정보를 검색하므로, LLM은 항상 최신의 정보를 기반으로 답변을 생성할 수 있다.
    * **환각 감소**: RAG는 LLM이 답변의 근거를 제시하도록 유도함으로써, LLM이 잘못된 정보를 생성하거나 헛소리를 하는 환각 현상을 감소시킨다.  검색된 문서를 통해 답변의 신뢰성을 확보할 수 있다.
    * **도메인 특화**: RAG는 특정 도메인에 대한 지식을 외부 데이터베이스에 저장하고, LLM이 필요할 때 해당 정보를 검색하여 활용하도록 함으로써, LLM을 특정 도메인에 특화된 전문가로 만들 수 있다.

-   **주의**:
    * **검색 품질**: RAG 시스템의 성능은 검색 시스템의 품질에 크게 의존한다.  검색 시스템이 관련성이 낮은 문서를 반환하면, LLM은 부정확하거나 관련 없는 답변을 생성할 수 있다.
    * **인덱스 업데이트 주기**: 외부 데이터베이스의 정보를 최신 상태로 유지하기 위해서는 주기적인 업데이트가 필요하다.  인덱스가 오래된 정보를 포함하고 있으면, LLM은 잘못된 답변을 생성할 수 있다.

---

## 5. MCP (Model Context Protocol) 상세 분석

~~~mermaid
flowchart LR
    subgraph Client
        P[Prompt\nmcp://…]
    end
    P -->|URI 요청| S[Context Server]
    S -->|JSON·파일| M[LLM]
~~~

MCP (Model Context Protocol)는 LLM이 외부 데이터 소스에 안전하게 접근하기 위한 표준이다. 위 다이어그램은 MCP의 구조를 나타낸다.

-   **목적**: LLM이 사내 데이터베이스(DB), API 등을 표준화된 URI를 통해 안전하게 읽어올 수 있도록 지원하는 것이다.  MCP는 LLM이 다양한 외부 데이터 소스에 접근하여 필요한 정보를 얻고, 이를 바탕으로 작업을 수행할 수 있도록 하는 것을 목표로 한다.

-   **구성 요소**:
    * **Context Server**: 외부 데이터 소스에 대한 접근을 관리하고, LLM의 요청에 따라 필요한 정보를 제공하는 서버이다.  Context Server는 데이터 소스의 위치, 인증 정보, 접근 권한 등을 관리하며, LLM에게 안전하고 효율적인 데이터 접근 인터페이스를 제공한다.
    * **Context Card**: LLM이 요청하는 정보에 대한 메타데이터를 담고 있는 JSON 형식의 파일이다.  Context Card는 데이터의 위치, 형식, 내용 등에 대한 정보를 포함하며, LLM이 어떤 데이터를 요청해야 하는지, 그리고 어떻게 처리해야 하는지에 대한 지침을 제공한다.
    * **LLM Consumer**: MCP를 준수하는 LLM 애플리케이션이다.  LLM Consumer는 MCP URI를 사용하여 Context Server에 데이터를 요청하고, Context Card를 해석하여 데이터를 처리한다.

-   **작동 방식**:
    1.  LLM Consumer는 필요한 정보에 대한 MCP URI를 생성하여 Context Server에 요청한다.
    2.  Context Server는 요청받은 URI를 해석하고, 해당 데이터에 대한 Context Card를 반환한다.
    3.  Context Server는 Context Card에 명시된 위치에서 데이터를 읽어와 LLM Consumer에게 전달한다.
    4.  LLM Consumer는 Context Card에 따라 데이터를 처리하고, 이를 바탕으로 작업을 수행한다.

-   **장점**:
    * **데이터 접근 방식 통일**: MCP는 다양한 데이터 소스에 대한 접근 방식을 표준화함으로써, LLM 개발자가 여러 데이터 소스에 대한 접근 방식을 일일이 구현할 필요 없이, 일관된 방식으로 데이터에 접근할 수 있도록 한다.
    * **권한 및 버전 관리 용이**: MCP는 Context Server를 통해 데이터 접근을 관리하므로, 데이터에 대한 접근 권한 및 버전 관리를 효율적으로 수행할 수 있다.  LLM에게 필요한 데이터만 제공하고, 데이터 변경 이력을 추적하는 것이 가능하다.
    * **보안성 강화**: MCP는 데이터 접근에 필요한 인증 및 권한 정보를 Context Server에서 관리하므로, LLM 애플리케이션의 보안성을 강화할 수 있다.  LLM이 직접 데이터 소스에 접근하는 것을 방지하고, Context Server를 통해 안전하게 데이터를 제공한다.

---

## 6. A2A (Agent-to-Agent Protocol) 상세 분석

~~~mermaid
sequenceDiagram
    participant A as Agent A
    participant B as Agent B
    A->>B: GET /.well-known/agent.json
    A->>B: POST /a2a/v1/messages {task}
    B-->>A: 진행 상황을 스트림 형태로 전달한다.
    B-->>A: 최종 결과를 반환한다.
~~~

A2A (Agent-to-Agent Protocol)는 여러 AI 에이전트가 협업하기 위한 통신 규칙이다. 위 시퀀스 다이어그램은 A2A의 기본적인 통신 흐름을 보여준다.

-   **목적**: 다양한 AI 에이전트가 각자의 기능을 분담하여 협업할 수 있도록 지원하는 것이다.  A2A는 LLM 기반 에이전트들이 서로 정보를 교환하고 작업을 요청하며, 복잡한 작업을 함께 처리할 수 있도록 하는 것을 목표로 한다.

-   **주요 요소**:
    * **Agent Card**: 에이전트의 이름, 설명, 제공하는 기능 목록, API 엔드포인트 등의 정보를 담고 있는 JSON 형식의 파일이다.  Agent Card는 다른 에이전트에게 자신의 존재와 능력을 알리는 역할을 한다.
    * **JSON 메시지**: 에이전트 간에 주고받는 모든 메시지는 JSON 형식으로 표현된다.  JSON은 다양한 프로그래밍 언어에서 쉽게 처리할 수 있는 구조화된 데이터 형식이다.
    * **스트림 이벤트**: 에이전트가 작업을 수행하는 동안 진행 상황을 실시간으로 다른 에이전트에게 알리기 위해 스트림 형태의 이벤트를 사용한다.  스트림 이벤트는 작업의 진행률, 중간 결과, 발생한 오류 등의 정보를 담을 수 있다.

-   **작동 방식**:
    1.  에이전트 A가 에이전트 B에게 작업을 요청하기 위해, 먼저 에이전트 B의 Agent Card를 요청한다.  이는 에이전트 A가 에이전트 B의 능력과 API 정보를 파악하기 위함이다.
    2.  에이전트 A는 에이전트 B에게 POST 요청을 통해 작업 내용과 필요한 데이터를 담은 JSON 메시지를 전송한다.
    3.  에이전트 B는 작업을 수행하면서 진행 상황을 스트림 형태의 이벤트로 에이전트 A에게 전달한다.  이는 에이전트 A가 작업 진행 상황을 모니터링하고, 필요한 경우 작업을 중단하거나 수정할 수 있도록 하기 위함이다.
    4.  에이전트 B는 작업이 완료되면 최종 결과를 JSON 형식의 메시지로 에이전트 A에게 반환한다.

-   **예시**: 여러 에이전트가 협력하여 문서를 작성하는 시나리오를 생각해 볼 수 있다.
    * "초안 작성 봇"은 주어진 주제에 대한 초안 문서를 작성한다.
    * "교정 봇"은 초안 문서를 검토하고 문법 오류나 오탈자를 수정한다.
    * "발송 봇"은 최종 문서를 지정된 수신자에게 이메일로 발송한다.

    이러한 시나리오에서 각 봇은 A2A를 통해 서로 필요한 정보를 교환하고 작업을 요청하며, 문서를 작성하는 전체 과정을 자동화할 수 있다.

---

## 7. 회고

본 보고서에서는 PR 코드 리뷰 자동화의 필요성과 PR Agent의 개요, 그리고 PR Agent와 관련된 핵심 개념인 RAG, MCP, A2A에 대해 조사하고 분석하였다.  LLM 기반 코드 리뷰 자동화는 개발 프로세스의 효율성을 향상시키고 코드 품질을 높이는 데 중요한 역할을 할 수 있다.  PR Agent는 이러한 자동화를 위한 도구이며, RAG, MCP, A2A와 같은 기술들을 통해 더욱 확장되고 발전될 수 있다.

이번 조사를 통해 얻은 주요 내용은 다음과 같다.

* LLM을 활용한 코드 리뷰 자동화는 개발 프로세스의 효율성을 크게 향상시킬 수 있으며, 코드 품질 향상에도 기여할 수 있다는 것을 알게 되었다.
* PR Agent는 다양한 LLM을 지원하며, 여러 환경에 유연하게 설치할 수 있는 오픈 소스 도구로서 높은 잠재력을 가지고 있다는 것을 확인하였다.
* RAG는 LLM이 외부 지식 소스를 활용하여 답변을 생성하는 기술로, LLM의 답변 정확도를 높이고 환각 현상을 감소시키는 데 기여한다.  특히, 최신 정보나 특정 도메인 지식이 필요한 경우에 유용하게 활용될 수 있음을 알 수 있었다.
* MCP는 LLM이 외부 데이터 소스에 안전하게 접근하기 위한 표준으로, 다양한 데이터 소스에 대한 통합된 접근 방식을 제공하고, 데이터 접근 권한 및 버전 관리를 용이하게 한다.  이는 LLM이 기업 내부의 다양한 데이터를 활용하여 더욱 풍부한 컨텍스트 정보를 바탕으로 작업을 수행할 수 있도록 지원한다.
* A2A는 여러 AI 에이전트가 협업하기 위한 통신 규칙으로, LLM 기반 에이전트들이 복잡한 작업을 함께 처리할 수 있도록 한다.  각 에이전트는 특정 분야의 전문성을 가지고 있으며, A2A를 통해 서로 정보를 교환하고 작업을 분담함으로써, 더욱 효율적으로 목표를 달성할 수 있음을 확인하였다.


이번 조사를 통해 얻은 지식을 바탕으로, 앞으로 진행될 PR Agent 관련 실습을 더욱 효과적으로 수행하고, 의미 있는 결과를 도출할 수 있도록 노력할 것이다.




> *작성자: Seungmin Lee | Group 3 | OSSCA PR Agent Mentoring 2025*