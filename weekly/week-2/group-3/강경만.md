# RAG

## **RAG이란?**

- *Retrieval-Augmented Generation*, 한국어로는 *검색 증강 생성*)은 대형 언어 모델(LLM)의 응답 생성 과정에 외부 지식 검색 단계를 통합함으로써 **보다 정확하고 신뢰할 수 있는 응답을 만들어내는 기술**
- LLM이 자체 파라미터에 저장된 지식만으로 답변하는 대신, **질의와 연관된 외부 데이터 소스에서 정보를 찾아와 활용하도록** 하는 접근법

## LLM과의 차별점 및 RAG의 필요성

### 기존 LLM의 한계

- 학습 시점 이후에 발생한 새로운 정보나 특정 전문 도메인의 사설 데이터에는 접근할 수 없다.
- 모델이 “암기한” 지식만으로 답을 구성하기 때문에 발생하는 환각 현상.(그럴듯한 답변 내놓기)

→ RAG: **필요한 정보를 그때그때 신뢰할 수 있는 소스로부터 찾아와** 모델의 응답 생성에 반영합니다.

### RAG의 필요성

**1. 최신 정보 활용 및 지식 확장:** RAG 시스템에서는 **질문 시점의 최신 데이터나 특정 DB의 자료를 즉시 검색하여 활용**할 수 있습니다. 이는 모델을 **동적으로 업데이트**하는 효과를 내며, **LLM의 지식 한계를 보완**합니다.

**2. 향상된 정확성 및 환각 감소:** 모델이 외부에서 찾은 **근거 자료를 바탕으로 답변**하게 되므로, **사실적 정확도**가 높아지고 근거 없이 추측하는 현상이 줄어듦. 답변과 함께 출처 제시 가능

**3. 효율적인 지식 업데이트 (비용 및 시간 절감)**

- LLM: 새로운 지식을 모델에 반영 위해선 다시 학습(파인튜닝)시켜야
- RAG를 쓰면 **모델을 재훈련하지 않고도** 지식베이스만 업데이트하면 됨.
- **신규 데이터 도입이 용이**, 필요에 따라 **지식 소스를 교체 또는 추가**하여 곧바로 응답에 반영할 수 있다는 장점.[blogs.nvidia.com](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/#:~:text=That%20makes%20the%20method%20faster,new%20sources%20on%20the%20fly).

**4. 모델 경량화 및 성능 향상**

- 기존 방식만으론 모든 지식을 모델 파라미터에 넣으려면 거대 모델이 필요
- RAG를 사용하면 **상대적으로 작은 LLM으로도 방대한 지식에 접근**해 답변 가능 [arxiv.org](https://arxiv.org/abs/2002.08909#:~:text=However%2C%20this%20knowledge%20is%20stored,that%20considers%20millions%20of%20documents)[arxiv.org](https://arxiv.org/abs/2002.08909#:~:text=,such%20as%20interpretability%20and%20modularity).
- 이는 **파라미터 크기에 비해 더 똑똑한 모델**을 만드는 방법으로 볼 수 있음

**5. 개발자 통제력 및 보안**

- RAG는 개발자가 **모델이 참고할 지식의 범위와 출처를 통제**할 수 있게 해줍니다[aws.amazon.com](https://aws.amazon.com/what-is/retrieval-augmented-generation/#:~:text=More%20developer%20control).
    - 민감한 내부 정보에 접근할 때 **권한 관리**를 적용
    - 특정 출처(예: 공식 문서)만 활용하도록 제한할 수 있습니다.
    - 모델 출력에 잘못된 정보가 포함될 경우, **어떤 자료를 잘못 검색했는지 추적**하여 해당 부분만 수정하는 등 **디버깅과 개선이 용이.**
- 일반 LLM의 잘못된 출력은 원인을 모델 내부에서 찾기 어려워 수정이 훨씬 까다로움

### RAG의 작동 원리

- 사용자의 질의에 따라 관련 정보를 **지식 소스**에서 검색하여 **LLM 프롬프트**에 추가하고, LLM이 최종 응답을 생성한다

### 일반적인 절차

1. **지식베이스 구축 (오프라인 사전준비)**
    1. LLM이 참고할 지식 소스 준비 → 효율적 검색을 위해 작은 청크 단위로 분할, 각 청크 별 임베딩 벡터 계산, 벡터 DB에 저장 → 검색용 index(색인): 일종의 지식 라이브러리화
2. **질의 임베딩 및 검색**
    1. 사용자가 질문을 입력 → **질의문을 임베딩 모델을 통해 벡터로 변환**
    2. 벡터 DB에서 **질의 벡터와 가장 유사한 벡터들**을 빠르게 검색 → 관련성 높은 상위 **N개의 문서 조각 찾음 (벡터 유사도 검색)**
        1. 벡터 유사도 검색이기에 정확히 동일 단어 아니더라도 관련 내용이라면 결과로 찾아낼 수 있음.
        2. (ex.사용자가 “우리 회사 연차 정책이 어떻게 되지?”라고 물으면 사내 지식베이스에서 “연차 규정” 문서를 찾아 반환)
3. **프롬프트 보강 및 생성 요청**
    1. 검색 단계에서 얻은 상위 **관련 문서 조각들**을 **원본 질문과 함께 LLM에 입력**. 
        1. **프롬프트 증강(prompt augmentation)** 또는 **컨텍스트 주입**: 원래의 사용자 프롬프트에 **추가 컨텍스트**로서 해당 문서 내용들을 첨부하거나, 프롬프트 템플릿에 따라 인용 형태로 삽입합니다.
            1. LLM이 참고해야 할 자료를 명시적으로 제공하는 것입니다. 
            2. LLM이 이러한 **증강된 프롬프트**를 받으면, 내부 지식과 함께 제공된 외부 지식을 모두 활용하여 최종 답변을 생성. 
4. **LLM 응답 생성:** LLM은 주어진 질문 + 문서 컨텍스트를 바탕으로 **최종 답변을 생성**합니다. 
    1. LLM은 컨텍스트로 주어진 내용을 마치 자신이 이미 알고 있었던 것처럼 활용하여, **질문에 대한 풍부한 답변**을 만들어냅니다

## RAG의 구성 요소

크게 **Retrieval(검색)** 부분과 **Generation(생성)** 부분으로 나눌 수 있으며, 이를 실현하는 몇 가지 핵심 구성 요소는 다음과 같다.

#### 임베딩 모델 (Embedding Model)

- 역할: 텍스트를 수치화된 **고차원 벡터로 변환**하는 역할.
    - 사용자의 질문과 지식베이스의 문서들을 모두 같은 임베딩 공간에 투영하여, **의미적으로 유사한 내용은 벡터 공간에서도 가까이 위치**하도록 합니다.
- 효과: 나중에 벡터 간 유사도를 계산함으로써 관련 문서를 찾을 수 있습니다.
- 활용하는 것:  주로 **사전 학습된 딥러닝 언어 모델**을 활용하며, 문장 임베딩을 산출하도록 **파인튜닝된 모델**이나 **API 서비스**를 많이 사용합니다.
- 중요한 사항: **질의와 문서 임베딩 간의 코사인 유사도** 등이 **내용적 관련성**을 반영하도록 모델이 잘 학습되어 있어야 한다는 점입니다. 상황에 따라, 한국어 같이 특정 언어에 특화된 임베딩 모델이나 멀티링얼 모델을 선택하는 것도 고려해야 합니다.
- 작업
    - **문서를 적절한 크기로 분할:** 너무 큰 문서는 임베딩하기 전에 의미 단위로 잘라야 효율적이며, 보통 수백 단어 내외의 청크로 쪼개어 임베딩[stackoverflow.blog](https://stackoverflow.blog/2024/08/15/practical-tips-for-retrieval-augmented-generation-rag/#:~:text=Simple%20implementation%3A%20We%20can%20create,embedding%20model%20and%20LLM%20by).
    - 각 청크에는 해당 내용을 요약하는 **벡터 표현**이 부여되고, 이후 검색시 이 벡터들을 단위로 매칭하게 됩니다. 최종적으로 임베딩 모델은 RAG 파이프라인의 **기초 준비 작업**을 담당하며, **지식베이스를 벡터화하여 인덱싱**하는 역할을 한다고 볼 수 있습니다.
- 예시
    - **OpenAI의 *text-embedding-ada-002:*** 범용 문장 임베딩에 많이 쓰이는 모델로, 영어뿐 아니라 다양한 언어에 대해 1536차원의 벡터를 반환합니다.
    - **Cohere**나 **Azure AI** 등

### 벡터 데이터베이스 (Vector DB)

- **역할:** 임베딩한 문서 조각들을 저장하고, 추후 질의 벡터와의 **유사도 검색**을 고속으로 수행하는 **전용 데이터베이스**입니다.
    - 일반 데이터베이스와 달리 텍스트가 아닌 **고차원 벡터**를 저장하며, 수백만 개 이상의 벡터에 대해서도 실시간에 가까운 속도로 **최근접 이웃(nearest neighbors)**을 찾을 수 있도록 설계됩니다.
    - 벡터 DB는 RAG의 “두뇌 속 도서관”이라고 할 수 있으며, **적절한 인덱싱 기법**(예: HNSW, IVF 등)을 통해 **탐색 속도와 정확도의 균형**을 맞춥니다.
- 예시
    - **Faiss**(Facebook AI): 매우 널리 쓰이는 벡터 검색 라이브러리로, CPU/GPU에서 대용량 벡터의 근접 이웃 검색을 효율적으로 수행할 수 있습니다
    - Weaviate, Qdrant, Pinecone 등
- 벡터 DB는 RAG에서 **“어떤 문서가 관련 있는가?”를 빠르게 찾아주는 핵심 역할**을 하며, 시스템 성능의 상당 부분이 이 컴포넌트의 효율에 좌우됩니다.

### 검색기 (Retriever)

- **사용자 질문을 받아 임베딩하고, 벡터 DB를 조회하여 상위 N개의 유사 문서를 반환하는 일련의 과정을 캡슐화한 것**
    - 라이브러리나 프레임워크에서 Retriever라는 용어는 이러한 검색 기능을 하나의 인터페이스로 제공할 때 사용됩니다.
    - 예를 들어 LangChain 등의 프레임워크에서는 사용자가 어떤 백엔드를 쓰든 동일한 Retriever 인터페이스로 검색 결과를 얻을 수 있게 해줍니다.
- 역할: RAG의 앞단에서 **“어떤 정보가 필요한가”**에 답하는 역할을 합니다.
    - 잘 설계된 Retriever는 **정확도(정관련 문서를 빠짐없이 찾아내는지)**와 **정밀도(불필요한 문서를 거르는지)** 면에서 우수해야 함
    - 이를 위해 **하이브리드 검색**(벡터+키워드), **교차 인코더 재랭킹**, **BM25 대비 임베딩 모델 성능 향상** 등의 기법이 활용됩니다[stackoverflow.blog](https://stackoverflow.blog/2024/08/15/practical-tips-for-retrieval-augmented-generation-rag/#:~:text=,retrieve%20the%20most%20relevant%20data)[stackoverflow.blog](https://stackoverflow.blog/2024/08/15/practical-tips-for-retrieval-augmented-generation-rag/#:~:text=,properly%20incorporated%20into%20the%20prompt).
    - 다만 입문 단계에서는 우선 기본적인 **임베딩 유사도 기반 Retriever**를 구축한 뒤, 필요에 따라 이러한 기법을 추가해 성능을 개선하면 됩니다. Retriever의 결과로 선택된 문서들은 **스코어**(유사도 점수)에 따라 정렬되어 LLM에 전달되며, 이때 상위 N값이나 임계값 조정을 통해 얼마나 많은 컨텍스트를 포함할지 결정하게 됩니다.
- **질문 → (임베딩) → 문서 검색 → 관련 문서 반환**이라는 기능을 수행하며, RAG 파이프라인의 정보검색 엔진으로서 동작합니다.

### 생성 모델 (Generator, LLM)

- 최종적으로 사용자 질문에 대한 답변을 생성하는 **언어 모델(LLM)**입니다. RAG 파이프라인의 **후단**으로서, **Retriever가 찾아준 문서를 컨텍스트로 받아** 답변을 산출합니다
- 생성 모델은 해당 **프롬프트 내의 모든 정보를 활용해** 답변하기 때문에, 사전에 가지고 있는 지식뿐 아니라 **주어진 자료에 근거한 출력**을 내도록 유도됩니다. 예컨대, 질문에 대해 LLM 스스로는 확실치 않은 경우에도 Retriever가 제공한 문서에 답이 있으면 정확히 짚어낼 수 있습니다.
- 현대의 LLM들은 **프롬프트 내 정보에 상당히 의존**하는 경향이 있어, 컨텍스트로 관련 문서를 넣어주면 그 내용을 반영한 응답을 잘 생성합니다. 다만, LLM이 컨텍스트를 **얼마나 충실히 반영**하느냐는 프롬프트 작성과 모델 특성에 달려 있습니다.
- Generator는 궁극적으로 **RAG의 답변 품질을 결정짓는 요소**이며, 주어진 컨텍스트를 얼마나 잘 이해하고 일관된 자연어로 답변하는지가 관건입니다.

### RAG의 한계와 고려해야 할 점

- **검색 실패 시 한계:** Retriever가 **적절한 문서를 찾지 못하면** LLM도 여전히 부정확한 답변을 내놓을 수 있습니다. 즉, RAG의 성능은 검색 단계의 품질에 크게 의존합니다.
- **제한된 컨텍스트 길이:** LLM이 한 번에 처리할 수 있는 프롬프트 길이 한도가 있기 때문에, 검색해온 문서가 너무 방대하면 모두 담을 수 없습니다. 현재 많아야 수천 토큰 수준의 컨텍스트만 투입 가능하므로, **상위 N개 결과만 사용할 수 있고 나머지 정보는 버려**야 합니다.
- **추가 구성 요소로 인한 복잡성:** 순수 LLM API 호출에 비해 RAG 시스템은 **벡터 DB, 임베딩 모델, 데이터 파이프라인** 등이 추가로 필요하므로 **구축 및 운영 복잡성**이 증가합니다.
- **데이터 품질 및 편향:** RAG는 **쓰레기 같은 데이터에서 쓰레기 답변**이 나온다는, 전통적인 IT 격언을 공유합니다. 즉, 지식베이스의 데이터가 부정확하거나 편향되어 있으면 RAG의 출력도 그대로 그 영향을 받습니다LLM 자체의 편향에 더해 외부 데이터 편향이 겹칠 수 있으므로, **투입하는 지식의 정확도 검증**과 **필요한 정제 작업**이 중요합니다.
- **프롬프트 엔지니어링 및 맥락 통합:** LLM이 컨텍스트를 잘 활용하도록 하는 **프롬프트 구성도 기술적 과제**입니다[stackoverflow.blog](https://stackoverflow.blog/2024/08/15/practical-tips-for-retrieval-augmented-generation-rag/#:~:text=,properly%20incorporated%20into%20the%20prompt). 컨텍스트를 어떻게 서식화하여 넣을지, 답변에 출처를 표기하게 할지 등을 실험적으로 최적화해야 합니다. 많은 양일 경우 중간 내용이 묻히지 않도록 유의
- **보안 및 프라이버시:** RAG 시스템이 다루는 지식이 회사의 내부 기밀정보인 경우, 이를 벡터화해 DB에 넣고 LLM에 전달하는 과정에서 **정보 유출 위험**을 관리해야 합니다.
- **응답 일관성:** LLM은 제공된 문서 외에 자신이 원래 알고 있는 지식을 함께 활용하기 때문에 변형을 가할 수 있음. **원문을 정확히 인용해야 하는 상황**(예: 법률 조항 답변 등)에서는 문제가 됩니다. 이러한 경우 **생성 대신 검색결과를 추출만 하는** 접근 등 고려.  **RAG에서도 생성 모델의 특성을 이해하고 적절히 활용할 수 있는 범위**를 설정하는 것이 중요합니다.

## 대표적인 활용 사례

- 기업 내부 지식봇, 고객 지원 및 FAQ 챗봇, 의료/법률 전문 상담, 학술 연구 보조, 검색 엔진 보강 Q&A, 코드 어시스턴트

# MCP

## 1. 기본 개념과 핵심 원리

- MCP는 AI 모델과 외부 데이터 소스 또는 도구를 연결하는 개방형 표준 프로토콜.
    - 다양한 데이터 소스와의 표준화된 연결을 제공합니다.
- MCP의 주요 목표는 AI 모델을 고립된 상태에서 꺼내 현실 세계의 데이터와 도구에 직접 접근하게 함으로써, 더 관련성 높고 유용한 응답을 생성하도록 돕는 것입니다.

### 클라이언트-서버 구조

- **MCP 호스트**: AI 모델을 운용하는 애플리케이션 (예: Claude Desktop, AI 어시스턴트)
- **MCP 클라이언트**: 호스트 내부에서 동작하며 MCP 서버와 1:1 연결을 담당
- **MCP 서버**: 외부 데이터나 기능을 모델이 이해할 수 있는 형태로 제공

통신은 JSON-RPC 2.0 기반의 표준화된 메시지 교환을 통해 이루어지며, 이를 통해 서로 다른 언어나 플랫폼으로 구현된 클라이언트/서버라도 원활히 협력 가능

## 2. 기본 구조와 예시

1. **Resources(리소스)**: 모델이 참고할 읽기 전용 데이터 (파일 내용, DB 레코드 등)
2. **Tools(도구)**: 모델이 호출할 수 있는 기능 또는 함수 (계산, API 요청 등)
3. **Prompts(프롬프트)**: 모델에게 제공하는 특정 지시나 템플릿

### MCP 동작 흐름:

1. 클라이언트-서버 연결 및 기능 교환
2. 모델의 질의 처리
3. 필요시 MCP 서버 호출
4. 결과를 모델에 제공
5. 모델이 최종 응답 생성

## 3. 장점과 한계

### **장점**

- **표준화**: 다양한 데이터 소스와 도구에 대한 접근을 한꺼번에 표준화
- **확장성**: 개발자는 통합에 소모되는 시간을 줄이고 확장성 있는 AI 솔루션 구축 가능
- **안전성**: 모델이 외부 도구를 사용할 때 사용자 승인이나 사전 정의된 정책에 따라 진행
- **유연성**: 파일 시스템, Slack, Google Drive, 데이터베이스 등 다양한 시스템과 연동 가능

**실제 활용 사례:**

- 파일 시스템 액세스: 로컬 파일 읽기, 검색 기능 제공
- 업무 도구 통합: Slack, GitHub, Google Drive 등과 연결
- 데이터베이스 질의: SQL 쿼리 실행 결과를 AI 응답에 활용

### 한계

- 주 출처: https://blog.sshh.io/p/everything-wrong-with-mcp

#### 1. **보안 문제**
  - **로컬 코드 실행 위험**: 사용자에게 직접 코드 실행을 유도 → 악성 코드 노출 가능
  - **입력값 과신**: LLM이 전달한 입력을 그대로 실행 → 오작동/악성 동작 가능성

#### 2. **UX/UI 문제**

- **도구 위험도 구분 없음**: delete_files() 같은 위험한 도구도 무차별 허용 → 실수로 큰 피해 가능
- **비용 통제 불가**: 응답 크기 = 비용 → 큰 결과는 고비용 유발
- **비구조적 응답 제한**: 시각적 정보, 데이터 검증 어려움 → 오작동/불편

#### 3. **LLM 보안 취약**

- **강력한 프롬프트 인젝션 가능**: 도구가 system 프롬프트를 오염시킬 수 있음
- **이름/설명 변경 가능**: 사용자 속여 악성 도구 실행 유도 가능
- **민감 정보 노출 위험**: 도구 사용 중 의도치 않은 정보 유출 발생 가능

#### 4. **LLM의 근본적 한계**

- **LLM 성능 저하**: 도구 많아질수록 지시 처리 성능 하락
- **도구 사용 정확도 낮음**: 실제 성공률 낮고, 사용자 기대와 괴리 존재
- **문맥 제한**: 긴 작업 문맥 처리에 한계, 도중 중단 가능
- **범용 도구 정의 어려움**: 어시스턴트마다 다른 도구 설계 요구

# A2A
**Agent2Agent (A2A)** 프로토콜은 여러 AI 에이전트가 서로 **안전하고 효율적으로 상호작용**할 수 있도록 설계된 **개방형 표준.**

## 핵심 개념

다양한 환경에서 만들어진 자율 에이전트들이 서로 그리고 사용자와 자연스럽게 협업할 수 있도록 표준화된 통신 방식

→ A2A는 간단하고 일관된 방식으로 에이전트를 연결할 수 있게 도움

1. **기능 탐색 (Capability Discovery)**: 에이전트는 JSON 형식의 “에이전트 카드(Agent Card)“를 사용하여 자신의 기능을 알림.
2. **작업 관리 (Task and state Management)**: 클라이언트와 원격 에이전트 간의 통신은 작업 완료를 지향하며, 작업의 결과물은 “산출물(artifact)“로 표현.
3. **보안 협업 (Secure Collaboration)**: 에이전트는 엔터프라이즈 인증과 OpenAPI 기반의 권한 부여를 지원하여 보다 안전하게 컨텍스트, 응답, 산출물 또는 사용자 지침을 전달하여 협업할 수 있음.
4. **사용자 경험 협상 (User Experience Negotiation)**: 각 메시지에는 생성된 이미지와 같이 완전히 구성된 콘텐츠 조각인 “부분(parts)“이 포함되며, 사용자 인터페이스 기능에 대한 협상을 명시적으로 포함할 수 있음.

---

## 👥 실제 사례: 채용 후보자 찾기 (A real-world example: candidate sourcing)

1. **채용 담당 에이전트가 시작점**이 되어 전체 프로세스를 트리거
2. 첫 번째 에이전트는 LinkedIn 같은 외부 구직 플랫폼에서 **적합한 후보자 목록을 수집**
3. 두 번째 에이전트는 Google Calendar 등 일정 도구를 사용, **면접 일정 자동으로 조율**
4. 세 번째 에이전트는 외부 백그라운드 체크 시스템과 연결, **신원 및 경력 검증**을 수행

각각의 에이전트는 A2A 프로토콜을 기반으로 정보를 주고받으며, **사람의 개입 없이도 유기적으로 협력**하여 하나의 목표(채용)를 수행

## 요약

- A2A는 **AI 에이전트 간의 공통 언어와 협업 구조**를 제공하여, 다양한 앱과 도구들이 자동으로 연결되게 합니다.
  - → 현실 업무 흐름(예: 채용, 고객 지원 등)을 **완전히 자동화**할 수 있는 가능성 열림