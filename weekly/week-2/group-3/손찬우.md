# 2주차 개별과제

## 학습 목표

- 주제: RAG, MCP, A2A
- 공부하고 정리

## RAG

### 정의

> RAG(Retrieval-Augmented Generation)는 대규모 언어 모델의 출력을 최적화하여 응답을 생성하기 전에 학습 데이터 소스 외부의 신뢰할 수 있는 지식 베이스를 참조하도록 하는 프로세스입니다. 대규모 언어 모델(LLM)은 방대한 양의 데이터를 기반으로 학습되며 수십억 개의 매개 변수를 사용하여 질문에 대한 답변, 언어 번역, 문장 완성과 같은 작업에 대한 독창적인 결과를 생성합니다. RAG는 이미 강력한 LLM의 기능을 특정 도메인이나 조직의 내부 지식 기반으로 확장하므로 모델을 다시 교육할 필요가 없습니다. 이는 LLM 결과를 개선하여 다양한 상황에서 관련성, 정확성 및 유용성을 유지하기 위한 비용 효율적인 접근 방식입니다.
> [AWS](https://aws.amazon.com/ko/what-is/retrieval-augmented-generation/)

- RAG = 검색 증강 생성(Retrieval-Augmented Generation)
- **검색(Retrieval)**
  - 사용자의 질의에 대해 외부 데이터 소스(예: 문서, 데이터베이스, 웹사이트 등)에서 관련 정보를 검색
- **증강(Augmented)**
  - 검색된 정보를 LLM의 입력에 추가하여 모델의 응답 생성을 보완
- **생성(Generation)**
  - 검색된 정보를 기반으로 LLM이 사용자의 질의에 대한 응답을 생성
- RAG(Retrieval-Augmented Generation) = 검색 증강 생성 = 사용자 질문에 대해 **외부 지식 기반**에서 관련 정보를 검색해 **응답 생성을 보완**하고, 답변의 정확성과 품질을 높이는특지 방법

### 특징

RAG는 외부 지식 활용에 중점을 두기에 다음과 같은 특징을 가진다:

1. **최신 정보 활용**
    - LLM은 학습 시점 이후의 데이터를 반영하지 못한다.
    - 학습 비용과 한계로 인해 LLM을 매번 최신 상태로 재학습하는 것은 어렵다.
    - RAG는 외부 데이터(예, 웹사이트)에서 최신 정보를 검색하여, 이를 기반으로 답변을 생성함으로써 최신 정보를 반영할 수 있다.
2. **정확하고 신뢰도 높은 답변 생성**
    - LLM은 때때로 사실이 아닌 내용을 생성하는 **hallucination** 문제를 가진다.
    - RAG는 외부 문서(출처가 명확한 문서)를 검색하고 이를 기반으로 답변을 생성하므로, 정확성과 신뢰성을 크게 향상시킬 수 있다.
    - 사용자에게 검색 근거를 함께 제시하면 답변의 검증 가능성도 높아진다.
3. **특정 도메인 정보 활용 (전문성 및 특수성)**
    - 일반적인 LLM은 범용적인 지식을 기반으로 답변을 생성한다.
    - 특정 산업, 조직, 전문 분야에 대해 질문할 경우, 범용 모델만으로는 깊이 있는 답변을 제공하기 어렵다.
    - RAG는 특정 도메인 관련 문서를 수집하여 지식 기반을 구축하고, 이를 검색하여 답변을 생성하기 때문에 전문성과 특수성을 갖춘 고품질 답변을 생성할 수 있다.

### 동작 원리

![RAG 흐름도](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/jumpstart/jumpstart-fm-rag.jpg)

- **문서 인코딩**
  - 데이터베이스에 저장된 모든 문서를 사전에 고차원 벡터로 변환하여 준비한다.
  - 변환된 문서 벡터는 벡터 데이터베이스에 저장된다.
- **질문 인코딩**
  - 사용자가 입력한 질문을 실시간으로 벡터로 인코딩한다.
  - 질문 인코딩은 별도의 질문 전용 인코더가 담당한다.
- **Dense Passage Retrieval(DPR) 사용**
  - **Dual Encoder** 구조를 사용한다.
    - 질문 인코더: 질문을 벡터로 변환
    - 문서 인코더: 문서를 벡터로 변환
  - **유사도 계산**은 질문 벡터와 문서 벡터 간 내적을 통해 이루어진다.
- **문서 선택**
  - 유사도 점수가 높은 문서 **상위 K개를 선택**한다.
  - 선택된 문서들은 이후 생성 단계에서 답변을 만들 때 활용된다.

## MCP

### 정의

> MCP는 애플리케이션이 LLM에 컨텍스트를 제공하는 방법을 표준화하는 개방형 프로토콜입니다. MCP는 AI 애플리케이션을 위한 USB-C 포트와 같습니다. USB-C가 다양한 주변기기와 액세서리에 기기를 연결하는 표준화된 방법을 제공하는 것처럼, MCP는 AI 모델을 다양한 데이터 소스와 도구에 연결하는 표준화된 방법을 제공합니다.
>[Anthropic](https://docs.anthropic.com/ko/docs/agents-and-tools/mcp)

- MCP = Model Context Protocol
- LLM 기반 애플리케이션이 외부 소스나 도구와 맥락을 주고받는 방식을 **표준화한 개방형 프로토콜**
- 2024년 말에 공개

### 특징

- **표준화**
  - 여러 다른 시스템들이 공통으로 따를 수 있는 **규칙**을 정의함.
  - "데이터 읽기/쓰기", "도구 호출" 방식을 **일관되게** 규정함.
  - 서버마다 별도 로직 없이, MCP 규칙 하나만 익히면 AI ↔ 데이터/도구 연결 가능.
  - HTTP/HTTPS처럼 **모든 도구를 통일된 방식**으로 연결할 수 있게 함.
- **개방형**
  - 특정 기업이나 단체에 **독점되지 않음**.
  - 누구나 MCP 서버나 MCP 클라이언트를 **자유롭게 구현**할 수 있음.
  - 특정 벤더에 종속되지 않고, 다양한 LLM/애플리케이션이 MCP를 사용할 수 있도록 설계됨.
- **프로토콜**
  - 서로 다른 시스템 간 **데이터를 주고받는 규칙이나 절차**를 명시함.
  - "어떻게 요청하고, 어떻게 응답할지"에 대한 **공식 약속**을 설정함.
  - 단순 API 스펙이 아니라, **대화 방식** 자체를 표준화함.
- **양방향 통신**
  - AI 모델이 외부로부터 **맥락(Context) 데이터**를 가져올 수 있음.
  - 필요시 외부 서비스에 **동작을 지시**할 수도 있음.
    - 예시: 파일 읽기뿐만 아니라 수정 요청 가능.
  - 통신 방식:
    - **로컬 환경**: 표준 입력/출력 파이프 사용
    - **원격 환경**: HTTP 스트리밍(Server-Sent Events) 방식 사용
  - 통신 프로토콜로 **경량화된 JSON-RPC 2.0** 사용하여 효율적임.
- **보안 및 권한 제어**
  - **호스트-클라이언트-서버** 아키텍처 도입.
  - AI 애플리케이션(호스트)이:
    - 어떤 MCP 서버에 연결할지 결정,
    - 어떤 권한을 부여할지 설정함.
  - 각 데이터 소스는 **별도 MCP 서버**로 분리되어 독립적으로 동작함.
  - 필요한 경우:
    - **사용자 인증**,
    - **권한 승인** 절차를 거칠 수 있음.
  - 사용자/기업은 **접근 제어 및 데이터 유출 방지**를 할 수 있음.
    - 예시: 파일 읽기는 허용하지만 쓰기는 차단.

### 동작 원리

![MCP 구조도](https://cf.channel.io/document/spaces/8276/articles/136693/revisions/242182/usermedia/67eb746dc5602ba25201)

구성 요소

- MCP 호스트
  - MCP를 통해 데이터에 접근하려는 주체 (클로드, ChatGPT 등)
- MCP 클라이언트
  - 호스트 안에서 서버와 1대1 연결 유지. 서버에 요청
- MCP 서버
  - 클라이언트의 요청을 받아서 정보를 실행하거나 동작을 실행함 (구글 드라이브, 슬랙 등)

*출처: [채널톡 블로그, Hyeri Jo, "MCP는 AI 업계의 표준이 될까요?”](https://channel.io/ko/blog/articles/what-is-mcp-52c77e72)*
