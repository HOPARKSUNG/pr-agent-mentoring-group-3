# 개인 과제 - RAG / MCP / A2A

## RAG(Retrieval-Augmented Generation)

### 개념

LLM의 한계를 보완하기 위해 외부 지식을 결합하는 개념이다.
일반적인 LLM은 학습되어 내재되어있는 지식으로만 답변하다보니,
학습 이후에 등장한 최신 정보 반영이 어렵고, 환각현상을 보이기 쉽다.
RAG는 이를 해결하고자 모델 외부에 신뢰할 수 있는 정보 소스를 두고,
질문에 맞는 관련 정보를 검색하여 응답 생성 시 참조하게 한다.
기본 구조는 질의에 대해 벡터 DB 등에서 관련 문서를 검색하고,
LLM 프롬프트에 이 문서를 추가해 답변 생성하는 형태로 이루어진다.

### 장점

응답의 사실성 향상과 최신성을 확보 할 수 있다.
모델이 최신의 신뢰할만한 사실에 접근하므로 업데이트되는 지식도 반영할 수 있다.
또한 출처도 제시할 수 있기 때문에 사용자 입장에서는 투명성과 신뢰도가 올라간다.
LLM의 환각을 줄이고, 파인튜닝 없이도 새로운 지식을 반영할 수 있어 운용비용을 줄일 수 있다.

### 단점

RAG는 정보 검색 단계의 성능에 의존하기 때문에 잘못된 문서를 가져오면 LLM도 잘못 대답할 수 있다.
유사도 검색은 편리하지만 자연어의 의미를 잘못 파악하여 비슷하지만 의도와는 다른 결과를 내줄 가능성이 있다.
RAG를 도입하면 기존 시스템보다 복잡해진다.
임베딩이나 검색 등 다양한 단계가 생기면서 확장성과 유지보수에 더욱 신경을 써야한다.
기존의 LLM보다 근본적으로 시간이 더 걸리게 된다.
임베딩을 할 때 청크 사이즈에 대한 고려가 이루어지는데 이것은 LLM이 맥락을 이해하는데에 있어 문제가 있다.
청크를 작게하면 맥락 이해가 힘들고, 청크를 크게하면 노이즈나 저장비용이 증가한다.

### RAG 아키텍쳐

![RAG 아키텍쳐](sanghyun_images/RAG.png)
출처 : https://aws.amazon.com/what-is/retrieval-augmented-generation/

#### 지식베이스를 구축하는 단계

1. 데이터를 수집한다.
2. 문서를 전처리하고 청크 단위를 설정하여 분할한다.
3. 텍스트 임베딩을 통해 정보들을 벡터로 변환한다.
4. 벡터 DB에 저장한다.

#### 질의응답이 이루어지는 단계

1. 사용자의 질문(자연어)을 벡터화한다.
2. 벡터 DB에서 유사 내용을 검색한다. (코사인 유사도 계산, kNN 알고리즘 등)
3. 찾은 문서(데이터)를 LLM 프롬프트에 포함시킨다.
4. LLM이 해당 내용으로 최종 답변을 생성한다.

### RAG 최적화에서 고려해야할 부분

1. 임베딩 모델의 성능
   - Query(목적)와 문서를 의미적으로 잘 매칭
   - 좋은 임베딩 모델은 질문과 진짜 관련 있는 문서를 잘 찾아낼 수 있게 해줌
2. 유사한 데이터를 찾을 때 적용시킬 수 있는 하이퍼파라미터값 설정
   - 탐색할 때 쓰는 세부 설정을 어떻게 조정하느냐도 성능에 큰 영향을 줌
   - ex) 검색해져 가져올 결과 개수, 가져올 최소 유사도 기준 등
   - 해당 값들을 적절하게 조정하지 않으면 과잉/과소 검색이 일어남
3. 문서 관련 청크 단위를 어떻게 구성할지
   - 청크가 너무 크면 검색정확도가 낮아지고, LLM 입력 토큰 길이를 초과할 수 있음
   - 청크가 너무 작으면 문맥이 깨져서 답변에 필요한 정보가 부족해짐

## MCP

## A2A
