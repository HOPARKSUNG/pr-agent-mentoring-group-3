# 📌 RAG(Retrieval-Augmented Generation)

## 1. 개요
RAG는 검색 기반 증강 생성 모델로, LLM의 한계를 보완하기 위해 제안됨. 모델은 고정된 파라미터만 사용하는 것이 아니라, 외부 지식 기반에서 관련 문서를 검색해 이를 컨텍스트에 포함하여 생성 결과의 정확성과 최신성을 높임.

## 2. 작동 방식
1. 사용자 입력 처리: 사용자의 질문을 벡터로 변환하여 의미 기반 검색이 가능하도록 함.
2. 외부 데이터 검색: 벡터화된 쿼리를 기반으로 벡터 데이터베이스에서 관련 문서를 검색. 이 데이터는 LLM이 훈련되지 않은 외부 소스(API, DB, 문서 등)에서 수집됨.
3. 프롬프트 보강: 검색된 정보를 LLM의 입력에 포함시켜, 기존 지식과 함께 응답 생성에 활용함.
4. 지식 업데이트: 외부 문서는 비동기 또는 주기적 방식으로 업데이트되며, 임베딩도 재생성되어 최신 정보를 반영함.

> 기업 내 문서 기반 챗봇
- 예를 들어, 한 직원이 "연차는 며칠 남았지?"라고 묻는다면, RAG 시스템은 회사의 휴가 정책 문서와 그 직원의 연차 기록을 외부 데이터에서 검색하고, 그 정보를 LLM에 전달하여 “현재 남은 연차는 5일입니다” 같은 응답을 생성함.

> 의료 정보 검색 챗봇
- 환자가 “최근 당뇨병 치료 가이드라인은 뭐야?”라고 질문하면, RAG는 최신 의료 논문과 가이드라인 문서를 검색하고, 이를 바탕으로 LLM이 요약된 정보를 제공함.

> e커머스 상품 문의 챗봇
- 고객이 “이 제품의 환불 정책이 뭐야?”라고 묻는 경우, 제품 관련 문서나 FAQ에서 관련 정보를 검색하고, LLM이 이를 바탕으로 자연어로 정리하여 알려줌.

> 개발자 지원 봇 (예: GitHub 문서 기반)
- 사용자가 “이 라이브러리에서 비동기 처리는 어떻게 해?”라고 물으면, 관련 공식 문서에서 비동기 처리 예시를 검색하고, LLM이 요약된 설명과 코드 예제를 응답으로 생성함.

## 3. 주요 특징
- LLM이 알지 못하는 외부 정보도 활용 가능
- 최신 정보, 도메인 지식 적용 가능
- hallucination(허위 생성) 방지
- 비용 효율적인 구현
  - 챗봇 개발은 일반적은 파운데이션 모델(FM)을 사용하여 시작하는데 조직 또는 도메인별 정보를 위해 FM을 재교육하는 데 드는 계산 및 재정적 비용이 많이 듦. RAG는 LLM에 새 데이터를 도입하기 위한 보다 비용 효율적인 접근 방식임.
- 사용자 신뢰 강화
- 개발자 제어 강화

## 4. 활용 사례
- 고객 지원 챗봇 (기술 문서 기반 응답 생성)
- 검색 기반 QA 시스템
- 법률, 의료, 논문 기반 질의 응답

## 5. 한계점
- 검색 결과의 품질에 의존
- 검색 + 생성의 latency 증가 가능성
- 정보 출처 불명확 시 신뢰도 문제

## 6. RAG가 중요한 이유
  LLM은 지능형 챗봇 및 기타 자연어 처리(NLP) 애플리케이션을 지원하는 핵심 인공 지능(AI) 기술임. 그러나 다음과 같은 알려진 문제점이 있음.
  - 답변이 없으면 허위 정보를 제공함
  - 사용자가 구체적이고 최신의 응답을 기대할 때 오래되었거나 일반적인 정보를 제공함
  - 신뢰할 수 없는 출처로부터 응답을 생성함
  - 용어 혼동으로 인해 응답이 정확하지 않음. 다양한 훈련 소스가 동일한 용어를 사용하여 서로 다른 내용을 설명함.

## 🔍 검색-증강 생성(RAG) vs 시맨틱 검색(Semantic Search)
| 항목 | 검색-증강 생성 (RAG) | 시맨틱 검색 |
|------|---------------------|--------------|
| **목적** | 검색한 정보를 LLM에 연결해 **최종 답변 생성** | 의미 기반으로 **관련 문서 검색** |
| **역할** | 검색 + 생성 (LLM이 응답 생성) | 의미 기반 검색 (키워드보다 의미에 집중) |
| **LLM과의 관계** | LLM이 중심, 정보를 받아 **답변 생성** | LLM에 **정보를 제공**하는 단계 |
| **출력 형태** | 자연어 문장 (예: “200만원 썼어요.”) | 문서 리스트나 텍스트 조각 |
| **개발자 관점** | 임베딩, 청킹 등 **복잡한 전처리 필요** | 대부분 **자동화됨**, 사용 쉬움 |
| **비유** | 정보를 찾아 **직접 대답해주는 비서** | 관련 정보를 찾아주는 **도서관 사서** |


# 📌 MCP(Model Context Protocol)
MCP는 애플리케이션이 LLM에 컨텍스트를 제공하는 방법을 표준화하는 개방형 프로토콜이다.
MCP는 AI 애플리케이션을 위한 USB-C 포트와 같습니다. USB-C가 다양한 주변기기와 액세서리에 기기를 연결하는 표준화된 방법을 제공하는 것처럼, MCP는 AI 모델을 다양한 데이터 소스와 도구에 연결하는 표준화된 방법을 제공합니다.

## 1. MCP를 왜 사용하는가?
MCP는 LLM을 기반으로 에이전트와 복잡한 워크플로를 구축하는 데 도움을 줌. LLM은 데이터 및 도구와 통합해야 하는 경우가 많으며, MCP는 다음과 같은 기능을 제공함.
  > ✅ 정보 조회를 넘어서 "일을 대신하게" 하려고
  - 기존의 LLM은 대답만 해줄 수 있음. 예를 들면:
    - "회의 일정 언제야?" → 📅 날짜를 알려줌
    - "코드 리뷰해줘" → 💬 리뷰 코멘트만 제공
  - 이걸 넘어서:
    - 🛠 "일정 직접 캘린더에 추가해줘"
    - 🛠 "GitHub에 직접 PR 남겨줘"
    - 🛠 "DB에서 고객 리스트 가져와줘"
  - 같은 ‘실제 행동’을 직접 시킬 수 있게 해주는 게 MCP
  > ✅ 자동화된 작업을 LLM과 연결하기 위해서
  - MCP는 말 그대로 LLM ↔ 도구(API) 간의 중계자 역할
  - 그래서 자동화를 이렇게 설계할 수 있음
    - "이메일로 문서가 오면 내용을 요약해서 슬랙에 보내줘"
    - → 메일 읽기 → 요약 → 슬랙 전송까지 모두 자동으로 가능.
- 사용자는 말만 하면 된다
  - 사용자는 "명령"을 자연어로 말하기만 하면, LLM + MCP + 도구가 합심해서 실행까지 처리해줘.
    - 📣 "회의록 요약해서 Notion에 정리해줘"
    - → ✅ Notion에 문서 자동 생성됨.

> ✅ 정리하면, MCP는 LLM이 정보를 말해주는 걸 넘어서, 직접 "행동"하게 함
- 💡 자동화 : 단순 응답이 아니라 행동까지 가능하게 함
- 🔌 연결성 : 다양한 도구(API 등)와 LLM을 표준 방식으로 연결
- 🧠 자연어 인터페이스 : 사용자는 복잡한 명령어 몰라도 자연어로 요청만 하면 됨
- ⚙️ 백엔드 추상화 : 사용자는 API나 인증 등 몰라도 되고, LLM이 MCP 통해 알아서 처리

## 2. MCP의 구성 요소
MCP는 클라이언트-서버 아키텍처를 따르며 구성 요소가 다음과 같이 이루어져있다.
> MCP 호스트(Host)
- Claude Desktop, IDE의 AI 어시스턴트, 채팅 애플리케이션 등과 같은 AI 모델을 운용하는 주체 애플리케이션
- 호스트는 사용자로부터 질문이나 명령을 받아 모델에게 전달 -> 모델의 응답을 사용자에게 보여주는 전체 흐름을 조율 / 내부에 MCP 클라이언트를 구동하여 MCP 서버들과 연결을 유지

> MCP 클라이언트(Client)
- 호스트 애플리케이션 내부에서 동작하며, 하나의 MCP 서버와 1:1 연결을 담당하는 컴포넌트
- 호스트는 여러 종류의 데이터를 쓰기 위해 여러 서버에 연결할 수 있는데, 이때 각 서버마다 전담 클라이언트가 필요
- MCP 클라이언트는 AI 모델 측(호스트 측) 에서 MCP 프로토콜을 구현하며, 서버로 요청을 보내고 응답을 받아 모델에 전달하는 역할

> MCP 서버(Server)
- 외부 데이터나 기능을 제공하는 측
- MCP 서버는 하나의 특정 서비스나 데이터 소스를 감싸서 모델이 이해할 수 있는 형태로 맥락(Context)을 제공
    - 파일 시스템 MCP 서버는 파일 읽기 기능
    - 날씨 MCP 서버는 날씨 정보 제공 기능
    - 데이터베이스 MCP 서버는 쿼리 실행 기능
- MCP 서버는 자신이 제공하는 기능을 표준화된 인터페이스로 정의하여 MCP 클라이언트 요청에 응답
- 하나의 호스트(예: AI IDE)는 여러 MCP 서버(파일, Git, 데이터베이스 등)에 동시에 연결해 다양한 맥락을 얻을 수 있음

> 맥락(Context) 요소
  - MCP 서버가 제공하는 실제 데이터와 도구들을 통칭하며 세 가지 범주로 나눔
    > Resources(리소스)
    - 모델이 참고할 읽기 전용 데이터
    > Tools(도구)
    - 모델이 호출할 수 있는 기능 또는 함수
    > Prompts(프롬프트)
    - 모델에게 특정 지시나 템플릿을 제공하는 문구

> 프로토콜(Protocol)
- MCP의 핵심인 통신 규약
- MCP는 통신에 JSON-RPC 2.0을 기반으로 한 표준 메시지 형식 사용
- MCP 프로토콜은 클라이언트와 서버가 어떻게 대화하고 상호작용하는지를 정의한 약속

## 3. MCP 동작 흐름
(1) 연결 및 기능 교환 → (2) 모델 질의 처리 → (3) 필요한 경우 MCP 서버 호출 → (4) 결과를 모델에 제공 → (5) 모델이 최종 응답 생산


# 📌 A2A(Agent2Agent)
## 1. A2A란?
Google이 오픈 소스로 공개한 A2A(Agent2Agent) 프로토콜은 AI 에이전트가 다양한 엔터프라이즈 플랫폼이나 애플리케이션 상에서 서로 통신하고, 안전하게 정보를 교환하고, 작업을 조정할 수 있도록 지원한다.

A2A는 에이전트에게 유용한 도구와 컨텍스트를 제공하는 Anthropic의 모델 컨텍스트 프로토콜(MCP)을 보완하는 개방형 프로토콜이다.

## 2. A2A 5가지 핵심 원칙
- 에이전트 기능 활용
- 기존 표준을 기반으로 구축(HTTP, SSE, JSON-RPC 등)
- 기본적으로 안전함
- 장기 실행 작업 지원
- 모달리티에 구애받지 않음(텍스트, 오디오, 비디오 스트리밍 등)

## 3. Agent2Agent (A2A) 구성 요소
| 구성 요소         | 설명 |
|------------------|------|
| **에이전트 카드 (Agent Card)** | `.well-known/agent.json` 위치에 존재하는 메타데이터 파일로, 에이전트의 기술, 엔드포인트, 인증 정보 등을 명시합니다. |
| **A2A 서버**       | A2A 프로토콜을 구현한 서버로, HTTP 엔드포인트를 통해 요청을 수신하고 작업을 실행합니다. 예: `/tasks/send`, `/tasks/sendSubscribe` 등 |
| **A2A 클라이언트**   | A2A 서버와 통신하는 애플리케이션 또는 다른 에이전트로, 작업 요청을 서버로 전송합니다. |
| **작업 (Task)**     | A2A의 기본 실행 단위로, 고유 ID를 가지며 메시지를 통해 생성되고 상태를 가집니다. |
| **메시지 (Message)** | 클라이언트와 에이전트 간의 통신 단위로, `role: user` 또는 `role: agent`로 구분됩니다. |
| **부분 (Part)**     | 메시지나 아티팩트 내부의 세부 콘텐츠로, 텍스트(`TextPart`), 파일(`FilePart`), 데이터(JSON, `DataPart`) 등을 포함합니다. |
| **아티팩트 (Artifact)** | 작업 중 생성된 출력물로, 파일, 구조화된 데이터 등의 결과물이며, Part들로 구성됩니다. |
| **스트리밍 (Streaming)** | 장기 실행 작업의 경우, SSE(Server Sent Events)를 통해 실시간 진행 상황을 전송합니다. |
| **푸시 알림 (Push Notification)** | 클라이언트가 제공한 웹훅 URL로 작업 업데이트를 미리 전달할 수 있는 기능입니다. |

## 4. 작업 진행 흐름
1. 발견 (Discovery)
   클라이언트는 서버의 `.well-known/agent.json`에서 에이전트 카드를 가져와 메타데이터를 확인합니다.

2. 초기화 (Initialization)
   클라이언트는 초기 사용자 메시지와 고유한 작업 ID를 포함하여 `tasks/send` 또는 `tasks/sendSubscribe`로 요청을 보냅니다.

3. 처리 중 (Processing) 
   - **스트리밍(Streaming) 방식**: 서버가 SSE(Server Sent Events)를 통해 `TaskStatusUpdateEvent` 또는 `TaskArtifactUpdateEvent`를 실시간으로 전송합니다.  
   - **비스트리밍(Non-Streaming) 방식**: 서버가 요청을 수신한 후 동기적으로 작업을 처리하고, 응답에 최종 결과를 포함시켜 반환합니다.

4. 상호작용 (Interaction, 선택 사항)
   작업이 `input-required` 상태일 경우, 클라이언트는 동일한 작업 ID로 추가 메시지를 `tasks/send` 또는 `tasks/sendSubscribe`를 통해 보냅니다.

5. 완료 (Completion)
   작업은 최종적으로 `completed`, `failed`, `canceled` 중 하나의 상태에 도달하며 종료됩니다.



### 참고한 문서
- [RAG란 무엇인가요?](https://aws.amazon.com/ko/what-is/retrieval-augmented-generation/)
- [모델 컨텍스트 프로토콜(MCP)](https://docs.anthropic.com/ko/docs/agents-and-tools/mcp)
- [Deep Research Model Context Protocol](https://discuss.pytorch.kr/t/deep-research-model-context-protocol-mcp/6594)
- [A2A 레포 README.md](https://github.com/google/A2A)