# 1주차 개인 학습 정리

## 1. Git 공부 - rebase, cherry-pick, Github Flow
## 2. LLM - Transformer 

---

# Git 공부

## git rebase

### 개념
git rebase는 현재 브랜치의 커밋들을 다른 브랜치의 최신 커밋 뒤로 재배치하는 명령어이다.  
다른 브랜치에서 했던 내용들을 해당 브랜치에서 작업한 것처럼 보이게 정리해준다.

### 목적

- 브랜치를 최신 상태로 업데이트하여 병합 충돌 가능성을 줄임
- PR 전 커밋 히스토리를 깔끔하게 정리 (쓸모없는 커밋 제거, 메시지 정리)
  - 여러개의 산만한 커밋들을 묶어서 하나의 커밋으로 정리하는 것이 가능
- 병합 커밋 없이 선형적 히스토리를 유지하여 가독성 향상

### merge와의 차이점

| 항목               | git merge                                     | git rebase                                  |
|--------------------|-----------------------------------------------|----------------------------------------------|
| 동작 방식           | 브랜치를 병합하고 merge commit 생성             | 커밋을 복사하여 다른 브랜치 위에 재배치        |
| 커밋 해시           | 유지됨                                        | 변경됨 (새 커밋으로 다시 생성)                 |
| 히스토리 형태       | 브랜치 흔적이 남는 트리형                       | 한 줄로 깔끔한 선형 히스토리                   |
| 충돌 처리 방식       | 병합 시 한 번                                   | 커밋별로 순차적으로 발생할 수 있음             |
| 협업 안정성         | 안전 (공유 브랜치 사용 가능)                   | 위험 (push 후 force 필요)                     |
| 주로 사용하는 경우   | 공유 브랜치 병합, 팀 전체 작업 병합             | 개인 브랜치 정리, PR 전 커밋 다듬기            |

---

## git cherry-pick

### 개념
특정 커밋 하나 또는 여러 개를 선택하여, 현재 브랜치 위에 복사하여 적용하는 명령어이다.  
브랜치 전체가 아닌 선택한 커밋만 가져올 수 있기 때문에, 기능 일부나 버그 픽스만 따로 적용할 때 유용하다.

### 목적

- 특정 커밋만 선택적으로 적용하고 싶을 때 사용
  - 예: 기능 전체가 아닌, 특정 버그 수정 커밋만 다른 브랜치에 적용하고 싶은 경우
- 긴급 수정 상황에서, 안정된 브랜치에 필요한 커밋만 반영 가능
- PR 없이 선별적으로 기능을 테스트하거나 백포팅 할 때 활용

---

## GitHub Flow

### 개념
GitHub Flow는 브랜치 전략으로,  
항상 배포 가능한 상태를 유지하는 `main` 브랜치를 중심으로,  
기능 단위의 브랜치 생성 → 작업 → Pull Request → 코드 리뷰 → 병합의 사이클을 반복하는 방식이다.

작고 빠른 배포, 자동화된 테스트, 지속적인 통합(CI)에 최적화되어 있으며,  
스타트업, 웹 서비스, 오픈소스 프로젝트 등 빠르게 변화하는 환경에 잘 어울린다.

### 목적

- **항상 안정된 main 브랜치 유지**  
  → main에 직접 커밋하지 않고, 반드시 Pull Request를 통해 병합
- **기능, 버그 수정 등 작업 단위마다 브랜치 생성**  
  → 개별 기능/수정 작업이 독립적으로 진행 가능
- **빠르고 반복적인 배포 사이클 구현**
  → 코드 리뷰와 CI 테스트를 거쳐 작은 단위로 자주 배포

### 브랜치 네이밍 규칙 예시

| 브랜치 유형     | 목적                     | 예시                                |
|------------|--------------------------|-------------------------------------|
| `feature/` | 새로운 기능 개발           | `feature/login-form`, `feature/chat` |
| `fix/`     | 버그 수정                 | `fix/login-redirect`, `fix/css-bug` |
| `hotfix/`  | 긴급 수정 (실제 프로덕션 대응) | `hotfix/payment-error`               |
| `docs/`    | 문서 작성/수정             | `docs/api-guide`                     |
| `refactor/` | 리팩토링 작업              | `refactor/user-service`             |
| `test/`    | 테스트 관련 작업            | `test/user-validation`              |

### 특징
- 모든 작업은 PR을 통해 진행된다   
   → main에 직접 푸시하는 것을 금지하는 룰을 설정해야함
- 병합 전략은 보통 `Squash Merge` 또는 `Rebase and Merge`를 선호
- 기능 단위 브랜치가 오래 지속되지 않도록 주의해야한다
   → 오래 지속될수록 충돌 가능성이 높아짐


---

# LLM - Transformer

## 🔶 Encoder

Transformer의 Encoder 블록은 총 두 개의 주요 레이어로 구성되어 있다:

1. **Self-Attention Layer**
2. **Feed Forward Layer**

이 외에도, **Multi-Head Attention**과 **Positional Encoding**,

그리고 **Residual Connection + Layer Normalization** 같은 보조 구조가 함께 동작한다.

---

### 🔷 Self-Attention Layer

입력 문장 내 **다른 위치에 있는 단어들과의 관계**를 고려해

현재 단어를 더 잘 인코딩할 수 있도록 하는 구조다.

즉, 단어와 문맥을 연결하는 핵심 과정이다.

### Self-Attention 동작 과정

1. **Query, Key, Value 벡터 생성**
    
    각 단어 임베딩에 학습된 가중치 행렬을 곱해 **Query, Key, Value 벡터**를 생성한다.
    
    - Query: 지금 단어가 **어떤 문맥을 찾고 싶은지**
    - Key: 각 단어가 **어떤 문맥 특성을 갖는지**
    - Value: 해당 단어가 **실제로 갖고 있는 정보**
2. **유사도 점수 계산**
    
    현재 단어의 Query와 모든 단어들의 Key를 내적해 **어느 단어와 가장 관련 있는지** 점수로 나타낸다.
    
3. **스케일링**
    
    점수가 너무 커지는 걸 방지하기 위해, Key 벡터 차원의 제곱근으로 나눈다.
    
    → 그래야 softmax 적용 시 출력이 0이나 1에 치우치지 않고,
    
    → **기울기가 사라지지 않아서 안정적인 학습**이 가능해진다.
    
4. **Softmax 정규화**
    
    점수를 확률처럼 정규화해,
    
    각 단어가 얼마나 집중 대상이 되는지 결정한다.
    
5. **Value 가중합 계산**
    
    각 Value 벡터에 softmax 점수를 곱한 뒤, 관련성이 높은 단어의 정보를 더 크게 반영한다.
    
6. **출력 생성**
    
    모든 Value 벡터의 가중합을 통해
    
    현재 단어에 대한 Self-Attention 출력 벡터를 얻는다.
    
    이 결과는 다음 레이어인 Feed Forward로 전달된다.
    

---

### 🔷 Multi-Head Attention

Self-Attention을 **여러 개의 시각으로 동시에** 수행하는 구조다.

Transformer에서는 보통 **8개의 attention head**를 사용한다.

### Multi-Head Attention의 이점

1. **자기 자신에 집중되는 현상 보완**
    
    단일 attention에서는 종종 단어가 자기 자신에만 집중하게 되는데,
    
    여러 head를 사용하면 **다양한 문맥 요소에 분산해서 주의**를 줄 수 있다.
    
2. **다양한 표현 공간 제공**
    
    각 head마다 **다른 Q/K/V 가중치 세트**를 사용해,
    
    동일한 단어도 **서로 다른 관점의 문맥 표현**을 학습하게 된다.
    

### 구조 흐름

- Self-Attention 계산을 **8번 독립적으로 수행**
- 각 head의 출력을 **concatenate**한 뒤
- 출력 가중치 행렬을 곱해 **최종 하나의 벡터**로 만든다

이 과정을 통해 **하나의 단어 표현 안에 다양한 문맥 요소**를 담을 수 있게 된다.

---

### 🔷 Positional Encoding

Transformer는 **입력 순서를 직접 다루지 않기 때문에**,

입력 단어의 **순서 정보를 인코딩**해서 임베딩에 더해줘야 한다.

### 위치 정보를 더하는 이유

- Transformer는 RNN처럼 순차적으로 입력을 보지 않는다
- 따라서 “내가 몇 번째 단어인지”를 알려주는 신호가 필요하다

### 구성 방식

- sin과 cos 함수를 기반으로 한 **수학적 패턴**을 따르며,
- 이 벡터는 각 단어 임베딩에 더해져
    
    **단어 간 상대적 거리나 위치 정보를 유지**할 수 있게 해준다
    

---

### 🔷 Feed Forward Layer

Self-Attention으로 문맥을 파악한 후,

각 단어의 벡터를 **독립적으로 가공하는 레이어**다.

두 개의 선형 변환과 비선형 활성화를 포함한 작은 MLP 구조로 되어 있다.

### 역할 및 구성

- **Self-Attention의 선형적 처리** 이후,
    
    **비선형성**을 추가해서 **표현력을 높인다**
    
- 각 단어마다 **독립적으로 적용**되며
    
    입력 → ReLU → 출력의 구조를 따른다
    

---

### 🔷 Residual Connection & Layer Normalization

Transformer의 모든 주요 서브 레이어(Self-Attention, Feed Forward)는

다음과 같은 구조를 따른다:

### 각각의 역할

| 구성 요소 | 설명 |
| --- | --- |
| **Residual Connection** | 입력 정보를 보존하여 **정보 손실을 줄이고** 역전파도 원활하게 함 |
| **Layer Normalization** | 출력의 분포를 정규화하여 **학습을 안정적으로 유지**함 |

→ 이 두 구조는 **Transformer의 깊은 네트워크가 잘 학습되도록 돕는 핵심 장치**다.

---

## 🔶 Decoder

Transformer의 디코더는 인코더의 출력과 이전 시점의 출력을 활용해 **다음 단어를 생성**하는 역할을 한다.

각 디코더 블록은 다음과 같은 세 가지 주요 서브 레이어로 구성된다:

- **Masked Self-Attention Layer**
- **Encoder-Decoder Attention Layer**
- **Feed Forward Layer**

그리고 인코더와 마찬가지로 **각 레이어는 Residual Connection + Layer Normalization**으로 감싸져 있다.

---

### 🔷 Self-Attention Layer (Masked)

디코더의 Self-Attention은 인코더와는 **약간 다르게** 작동한다.

**미래 단어를 보지 못하도록 마스킹**을 적용해 **오직 이전 단어까지만 참조 가능**하게 만든다.

### 마스킹

- 출력 시퀀스를 생성할 때 **미래 단어를 참조하는 건 허용되지 않으므로,**
    
    softmax 계산 전에 **미래 위치를 -∞로 설정하여 마스킹**한다.
    
- 이로 인해 해당 위치의 attention 값은 0이 되며, **현재 시점 이전의 단어들만 보고 결정**하게 된다.

이 구조는 **순차적인 단어 생성을 가능하게** 하며,

**문장 생성 시 언어의 흐름을 자연스럽게 학습**할 수 있도록 돕는다.

---

### 🔷 Encoder-Decoder Attention

이 레이어는 디코더에서만 존재하는 레이어로,

**인코더의 출력**을 참조하여 디코더가 **입력 문장의 특정 부분에 집중**할 수 있게 한다.

- **Query**는 디코더의 아래 레이어에서 가져오고,
- **Key / Value**는 인코더의 최종 출력에서 가져온다.

디코더는 자신이 만든 **Query**로, 인코더가 만든 입력 문장의 **Key / Value**와 비교해

**어디를 참조해야 할지를 계산**하는 구조다.

---

### 🔷 Feed Forward Layer

Self-Attention과 Encoder-Decoder Attention을 거친 후,

각 단어의 표현은 **Feed Forward Layer**를 통해 **비선형 가공**된다.

- 두 개의 선형 변환 + ReLU 활성화 함수로 구성
- 각 단어 위치에 대해 **독립적으로 동일한 FFN을 적용**

이후 **Residual Connection + Layer Normalization**을 통해 정보 손실 없이 다음 블록으로 전달된다.

---

## 🔷 Output: Final Linear + Softmax Layer

디코더의 마지막 출력은 **벡터 형태**로 나오기 때문에,

이를 실제 단어로 바꾸기 위해 두 개의 레이어가 추가로 사용된다.

1. **Linear Layer**
    - 디코더 출력을 **출력 단어 수만큼의 차원**을 가진 **logits 벡터**로 변환
2. **Softmax Layer**
    - logits 벡터를 확률 분포로 변환
    - 가장 높은 확률을 갖는 단어를 선택 → **해당 시점의 출력 단어로 사용**

---

## 🔷 Training 과정

Transformer의 학습은 **훈련용 정답 문장**과 **모델의 출력**을 비교하며 진행된다.

### 예시: "merci" → "thanks"

- 학습 초기, 모델의 가중치는 무작위이므로 **출력은 엉뚱한 확률 분포**를 만든다.
- 이 확률 분포와 실제 정답 단어의 **one-hot 벡터**를 비교하여 오차를 계산하고,
- 그 오차를 바탕으로 역전파를 통해 가중치를 조정한다.

### 손실 함수(Loss Function)

- **Cross-Entropy Loss**를 사용해 **예측 분포와 실제 분포의 차이**를 계산

---

## 🔷 번역 결과 생성 방식

Transformer는 한 번에 하나의 단어씩 예측하며 문장을 완성한다.

이때 출력 단어를 고르는 방법은 크게 두 가지다:

### 1. **Greedy Decoding**

- 각 스텝에서 **가장 확률이 높은 단어 하나**만 선택
- 단순하지만 전역 최적이 아닐 수 있음

### 2. **Beam Search**

- **여러 후보를 유지하며 탐색**
- 예: beam size = 2이면 매 시점마다 **가장 가능성 높은 2개의 시나리오**를 유지하고 확장
- 정확도는 높지만 속도는 느려짐
- beam size와 top_k는 **튜닝 가능한 하이퍼파라미터**

참고자료 : https://nlpinkorean.github.io/illustrated-transformer/