# ChatGPT의 작동 원리 이해하기 - 학습 요약

### [📝 상세 필기 보기](https://github.com/jihan-chillin/TIL/blob/main/LLM/LLM%20%EC%8B%AC%EC%B8%B5%EB%B6%84%EC%84%9D%20(Chat%20GPT%20%EC%9E%91%EB%8F%99%EB%B0%A9%EC%8B%9D).md)

- LLM의 기본 구조 및 작동 원리 이해
- Pre-training / Post-training / SFT의 구조와 흐름 정리
- Transformer 기반 모델의 한계와 활용 방식

---

## 1. Pre-Training
모델을 구축하기 위해 가장 비용이 큰 단계이고, <br/>
세부 단계는 아래와 같다.


| 단계           | 설명                                                       |
|----------------|------------------------------------------------------------|
| 데이터 수집    | 웹사이트, 위키피디아 등에서 다양한 텍스트 데이터 크롤링   |
| 데이터 정제    | 불필요한 정보 제거, 중복 콘텐츠 필터링, 언어 기준 정렬 등 품질 개선 작업 수행 |
| 토큰화 (Tokenization) | 문장을 의미 단위로 분할해 모델이 이해할 수 있는 토큰 형태로 변환 |
| 모델 학습      | 주어진 토큰 시퀀스에서 다음 토큰을 예측하는 방식으로 사전 학습 진행 |


### 토큰화 (Tokenization) 
가장 주요하다고 생각하는 개념은 토큰화(Tokenization)이다. 
토큰화는 방대한 원시 데이터를 효율적으로 처리하기 위해 채택된 방식으로, 이를 이해함으로써 모델이 어떻게 추론을 하는지 알 수 있다. 그래서 모델이 왜 완벽하지 않은 답변을 도출하는 지 알 수 있으며, 현재 사용되는 모델들을 더 효과적으로 활용하기 위한 프롬프트 설계 방법을 고민할 수 있으며, 더 나은 결과를 얻기 위한 접근 방식을 찾을 수 있게 된다.


### Transformer
> 토큰화된 데이터를 입력으로 받아 작동하는 모델 내부 구조 
- 입력: 토큰 시퀀스
- 출력: 다음 토큰의 확률 분포 -> 토큰 단위로 예측하여 결론 도출

<br/>

## 2. Post-Training
모델이 사용자의 명령을 인식하고 적절히 반응할 수 있는 LLM을 만들기 위해서는 적절한 Post-training 과정을 거쳐야한다.


### SFT (Supervised Fine-Tuning) vs RLHF (Reinforcement Learning from Human Feedback)

|항목 | SFT | RLHF |
|----|-----|-----|
|학습 방식 | 지도학습 (Supervised Learning) | 강화학습 (Reinforcement Learning)|
|데이터 출처 | 사람이 작성한 질문-답변 쌍 | 사람이 평가한 응답 순위|
|장점 | 빠른 튜닝, 명확한 정답 학습 | 사용자 선호 반영, 실전 대응력 향상|
|단점 | 표현 다양성 부족, 사람 편향 반영 | 보상 모델 편향, 학습 비용 큼|

## 3. 마무리하며

LLM의 동작 방식을 살펴보면서, 기술이 늘 자연을 모방해왔다는 사실을 다시금 상기하게 되었다.
영상에서 저자는 LLM에서 계산 과정을 수행하는 뉴런 구조가 생각보다 단순하다고 말했는데, 이를 들으며 복잡한 뉴런 구조를 가진 인간은 얼마나 정교한 유기체인가를 되새기게 되었다.

또한, 미래에 LLM의 뉴런 구조와 처리 방식이 고도화되어 인간 수준에 도달하게 되면, 인간이 감당할 수 없는 방대한 지식을 빠르게 습득하고 병렬적으로 처리하는 존재가 될지도 모른다는 생각에 묘한 감정을 느꼈다.
설령 LLM이 통계 기반 시뮬레이터에 불과하더라도, 그 입출력의 메커니즘은 결국 인간을 모방한 결과일 것이기에 한편으로는 약간의 두려움도 들었다. 아직 이 분야에 대해 잘 모르는 점도 이런 감정을 느끼게 만든 것 같다.