# 📌 LLM(Large Language Model)
**방대한 양의 텍스트 데이터를 기반으로 학습된 인공지능 모델**

자연어를 기반으로 텍스트를 이해하고 생성할 수 있어 자연어 대화, 자동 번역, 글쓰기 및 컨텐츠 생성, 코드 작성 등 다양한 작업에서 활용할 수 있다.
> LLM 예시:  
> OpenAI의 ChatGPT, Google의 Gemini(구 Bard), Meta Llama

<br>

## 🔍️ LLM 구축 과정
LLM은 크게 사전 훈련(Pretraining)과 후속 훈련(Post-training)의 두 단계로 구축된다.

### ✏️ 1. 사전 훈련(Pretraining)
**방대한 양의 인터넷 텍스트 데이터를 기반으로, 신경망이 다음 토큰을 예측하는 능력을 학습하는 과정**

- 전체 과정 요약
    - 데이터 수집 → 토큰화 → 신경망 학습 → 추론 가능 모델 완성
- 핵심 포인트
    - 인간처럼 언어를 "이해"하기보다는, 패턴을 기반으로 "확률적으로 다음 토큰을 예측"하며 동작한다.
    - 이 단계는 한 번만 수행되며, 결과물로는 텍스트 생성 능력을 갖춘 Base Model이 만들어진다.

<br>

Base Model은 문맥에 따라 자연스러운 텍스트를 생성할 수 있으나, 대화 흐름의 이해나 인간 친화적인 응답 능력은 부족하여 후속 훈련이 필요하다.

<br>

### ✏️ 2. 후속 훈련 (Post-Training)

**사전 훈련으로 만들어진 Base Model을 기반으로, 사용자의 질문에 더 잘 응답하고, 유용하고 안전한 답변을 생성하도록 만드는 과정**

- 전체 과정 요약
    - 지도 학습(SFT) → 인간 피드백 기반 강화 학습(RLHF) → 최적화를 위한 추가 튜닝 혹은 학습
- 핵심 포인트
    - Base Model을 사람 친화적으로 만드는 과정으로 단순한 텍스트 생성기에서 "상호작용 가능한 AI"로 발전한다.

<br>

#### 📝 2-1. 지도 학습(Supervised Fine-Tuning, SFT)
- 사용자의 질문에 “어떻게 답해야 하는지”를 모델에 제공
- 사람이 만든 질문-답변 쌍이나 대화 로그를 기반으로 모델을 미세 조정

이 과정을 통해 모델은 명령을 따르고 대화 구조를 이해하는 능력을 갖추게 된다.

<br>

#### 📝 2-2. 강화 학습(RLHF: Reinforcement Learning with Human Feedback)
- 모델의 출력을 더 친절하고, 사실 기반이며, 무해하도록 개선
- 보상 모델을 학습하고 다시 튜닝

이 과정을 통해 모델은 도움이 되고, 진실하며, 무해한 방향으로 응답하도록 개선된다.

<br>

#### 📝 2-3. 기타 후속 조정(선택적)
- 프롬프트 형식 학습: 사용자 입력을 더 잘 해석하도록 특수 토큰이나 포맷 학습
- 페르소나 설정: 특정한 말투나 역할을 강화
- 안전성 필터링: 부적절하거나 위험한 응답을 줄이기 위한 필터링이나 정책 반영

<br>

## 🔍️ 사고 모델(Thinker Model)
**문제 해결을 위해 스스로 사고/추론하는 모델**

- SFT 모델 vs 사고 모델
    - SFT 기반의 LLM은 주어진 질문에 대해 정답처럼 보이는 응답을 예시 기반으로 따라하도록 훈련되었다.
    - 반면, 사고 모델은 문제를 단계별로 사고하고, 중간 과정을 생성하면서 문제를 해결하도록 유도한다.
- 핵심 포인트
    - SFT 모델은 정답을 회상하는 방식에 가깝고, 사고 모델은 문제 해결을 위한 사고 과정을 시뮬레이션한다.

SFT 모델과 비교하여 보다 복잡한 문제를 해결하고 추론 과정이 투명하다.
