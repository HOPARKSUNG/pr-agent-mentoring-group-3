### LLM의 기본 개념과 작동 원리 정리

1. 웹 페이지 → 토큰 시퀀스 변환
	•	웹 페이지의 텍스트만 추출 (예: Navigation 등은 제거).
	•	텍스트는 UTF-8로 인코딩되고, 8비트 단위로 바이트 묶음 생성.
	•	바이트 시퀀스에서 반복되는 패턴을 찾아 토큰화.

⸻

2. 토큰화(Tokenization)의 이유
	•	단어 단위가 아닌, 더 작은 서브워드 단위(예: “un”, “happi”, “ness”)로 쪼개어 처리 가능.
	•	반복성과 통계성이 높은 단위를 사용하면 학습 효율이 올라감.
	•	대표적인 토크나이저: BPE(Byte Pair Encoding), SentencePiece

⸻

3. 토큰 예측 학습
	•	**토큰 윈도우(context window)**를 무작위로 선택하여 다음 토큰을 예측.
	•	이 과정은 **확률 기반(Softmax)**으로 동작:
	•	각 다음 토큰의 확률 분포를 만들고,
	•	가장 확률이 높은 토큰 or 샘플링 기반 선택.

⸻

4. 신경망 훈련 과정
	•	예측이 정답에 가깝도록 **가중치(파라미터)**를 조정.
	•	이때 사용되는 손실 함수는 Cross-Entropy Loss:
	•	실제 정답과 예측한 확률 분포의 차이를 줄이도록 함.
	•	이 모든 과정은 병렬 처리되어 큰 배치의 토큰들을 빠르게 학습함.

⸻

5. 기초 모델(Base Model)과 인스트럭트 모델 차이
	•	기초 모델은 고급 자동 완성처럼 동작.
	•	명확한 지시가 없으면 모호하거나 미묘한 답변을 줄 수 있음.
	•	예시 프롬프트:
```json
{
  "butterfly": "나비",
  "ocean": "바다",
  "teacher":
}
```
→ 자동 완성 방식으로 "선생님" 등을 예측해 채움.

6. Context Window (최대 토큰 길이)
	•	모델은 한 번에 처리할 수 있는 토큰 수가 정해져 있음.
	•	이 범위를 벗어나면 앞 문맥을 잊거나, 비용이 급증함.
	•	예: GPT-4는 최대 128k 토큰까지 확장 가능.