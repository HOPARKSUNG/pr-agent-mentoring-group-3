## 📚 대형 언어 모델(LLM) 작동 원리 이해하기

> ChatGPT는 어떻게 작동하는가? <br>
> (eng)[https://youtu.be/7xTGNNLPyMI] <br>
> (kor)[https://youtu.be/6PTCwRRUHjE] <br>

> 🔄 학습 진행 중 (강화학습 전까지 시청 완료)

<br>

## 1단계. Pre-training (사전 학습 단계)

> 인터넷에서 수집한 방대한 텍스트 데이터를 바탕으로 다음에 나올 단어를 예측하도록 학습

#### 데이터 수집 및 전처리

- 웹 크롤링하여 인터넷 문서 수집
- 필터링 (url 필터링 - 스팸/성인물 제외, 개인정보 제거, 중복 문서 제거 등)
  - 텍스트만 추출 (HTML에서 순수 텍스트만)
  - 언어 필터링: 언어 분류 후 다국어 비율 조정
  - 약 40TB 분량의 텍스트 데이터 확보

#### 토큰화(Tokenization)

- 텍스트 → 바이트 시퀀스 → 토큰 시퀀스

  - BPE(Byte Pair Encoding) 알고리즘: 가장 자주 나오는 바이트 쌍을 묶어 새로운 토큰 생성 → 시퀀스 압축<br>
  - 시퀀스 길이를 단축하고 의미 단위 유지
  - GPT-4가 약 10만 개 토큰의 토큰을 사용

#### 신경망 학습

- 토큰 시퀀스 입력 후 다음에 올 토큰 예측
  : 결국 훈련 데이터와 가장 일치하게 만드는 매개변수를 찾는 과정
- 손실 함수 (Loss)
  - 모델이 예측한 토큰과 실제 다음 토큰 차이 계산
  - Loss가 낮을수록, 다음 단어를 잘 예측한다는 의미 <br>
- 초기 가중치 랜덤 → 수천억 개의 파라미터 업데이트

#### 추론

- 사전 학습된 확률 기반으로 다음 토큰을 생성
- 항상 같은 결과가 나오는 것이 아니라 확률적인 결과
- 훈련 데이터와 완전히 일치X, 추론하는 것 - 훈련 데이터와 유사하게

#### Base Model

> 인터넷 데이터와 통계적으로 가장 유사한 토큰 시퀀스 생성

- 단순히 인터넷 흉내내서 토큰 생성, 튜닝되지 않은 원형 모델
- 정확X, 확률적 - 같은 질문에도 매번 다른 답이 나올 수 있음
- 때때로 학습한 내용을 재출력하기도 함 - 더 많이 샘플링된 경우<br>

| 구분                | 의미                     | 비유                                                           |
| ------------------- | ------------------------ | -------------------------------------------------------------- |
| **파라미터**        | 훈련 중 저장된 지식      | 오래전에 본 내용을 흐릿하게 기억하는 느낌 (vague recollection) |
| **컨텍스트 윈도우** | 현재 대화 중 주어진 정보 | 작업 기억 (working memory)                                     |

> 예) 2024년까지 학습한 모델도, 2025년 내용을 입력하면 바로 그것을 토대로 답할 수 있음 (context window에 정보가 주어지면 가능함.

#### 프롬프트 기법

- Few-shot prompting
  : 예시를 몇 개 넣고, 모델이 패턴을 학습해 유사한 출력 유도 <br>
  예) teacher: 선생님, cinamon: 계피, story:
- 역할 기반 프롬프트

  ```
  human: Hello!
  assistant: Hi there! How can I help you today?
  ```

  이런 구조의 프롬프트를 보내면 기본 모델로도 대화 어시스턴트처럼 동작하도록 최대한 유도 가능

## 2단계. Post-training (SFT, Supervised Fine-Tuning)

- Human Labeler가 만든 수천~수만 개의 Q&A 데이터셋 학습 <br>
  → 잘 짜인 대화 데이터셋은 수가 더 적고, 고품질 → 사전 학습 보다 짧은 시간, 낮은 비용으로 fine-tuning 가능
- 사람이 직접 쓴 이상적인 답변과 대화 구조를 학습

#### 대화 데이터 토큰화

- 모든 대화는 1차원 토큰 시퀀스로 표현됨
- 특수 토큰을 사용! ` <im_start>`, `<im_sep>`, `<im_end>` 등
  ```
  <im_start>user<im_sep>2 + 2는? <im_end>
  <im_start>assistant<im_sep>2 + 2는 4입니다. <im_end>
  ```
  - 예시) `<|im_start|>assistant<|im_sep|>` - 어시스트 차례임을 알려줌
  - 실제로 사용자에게는 보이지 않지만 컨텍스트 윈도우에 포함됨

## LLM의 한계와 극복 방법

#### 환각(Hallucination) 현상

- 모델은 모르는 사실도 “모른다”라고 말하지 않음
- 학습된 통계를 바탕으로 확률적으로 가장 그럴듯한 답변을 생성
  → 거짓 정보 생성 가능

#### 그렇다면 모델에게 모른다는 것을 어떻게 훈련시킬까? (Meta의 논문)

- 단락 데이터를 기반으로 질문을 생성하게 만들고, 이 질문에 모델이 스스로 답하게 함
- 3~5회 반복해서 질문 후 틀린 답을 연속해서 내면 <br>
  → “모른다”는 응답을 정답으로 학습시킴 → 지식의 경계 파악

#### 정확도 높이는 방법

- 방법 1) 웹 검색 기능

  - `<SEARCH_START>` 토큰 → 실제 검색 엔진에 전송됨
  - 검색 결과는 [ ] 형태로 모델 컨텍스트에 삽입 → 실제 답변에 참고됨
  - 사용자는 못 보지만, 모델은 실시간 웹 검색 데이터를 참조 가능
  - 신경망의 매개변수는 아마 한달 전 학습된 것, 컨텍스트 window의 토큰은 작업 기억과 같음
    - 컨텍스트 윈도우에 있으면 모델이 직접 접근 가능하니 더 잘 답변 가능

- 방법 2) Code Interpreter 사용

  - 모델이 직접 계산하지 않고, 코드로 문제 해결
  - 프롬프트에 “use code” 지시 → Python 코드 생성 → 실행 후 결과 리턴
  - 계산은 모델 보다 코드한테 넘기자

- 단계적 추론

  - 토큰당 처리 가능한 연산량이 제한되어 있기 때문에,
  - 모든 계산을 한꺼번에 모델에게 시키면 연산 자원 한계로 비효율적
  - 복잡한 문제는 단계별 reasoning 유도
  - Chain-of-Thought (CoT) Prompting

#### LLM의 자기 인식

- LLM은 기본적으로 자기 인식이 없음
- 그저 훈련 데이터를 바탕으로 통계적 추측만 할 뿐
- 하지만 하드 코딩된 데이터로 페르소나를 설정하거나,
- 시스템 메시지를 활용해 모델에게 자신이 누구인지 상기시켜줄 수도 있다. (이때 시스템 메시지도 숨겨진 토큰)

#### 추가적인 LLM의 한계

- 계산/암산 → Code Interpreter 사용하자
- 맞춤법: 모델은 텍스트를 토큰 단위로 이해하기 때문에 약함
- 문자수 비교: 마찬가지로 토큰 단위로 작업해서 정확도 낮음
- 숫자 비교 - 9.11 vs. 9.9와 같은 숫자 단순 비교도 어려워함.
