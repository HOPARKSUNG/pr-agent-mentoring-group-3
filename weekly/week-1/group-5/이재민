## 📚 대형 언어 모델(LLM) 작동 원리 이해하기

> ChatGPT는 어떻게 작동하는가? <br>
> (eng)[https://youtu.be/7xTGNNLPyMI] <br>
> (kor)[https://youtu.be/6PTCwRRUHjE] <br>

사전 학습 단계까지 학습 완료
---

 1단계. Pre-training (사전 학습 단계)
> 인터넷의 텍스트 데이터 바탕 -> 다음에 나오는 단어가 무엇일까에 대한 예층르 반복 -> LLM의 기본적인 언어 감각을 길러주는 단계

1) 데이터 수집 & 전처리 – 웹의 잡음을 제거하고 순수한 텍스트만 남기기

- 전 세계 웹사이트에서 텍스트 문서를 대규모로 크롤링함 (뉴스, 위키, 코드, 논문, 포럼 등)
- 이걸 그대로 쓰면 안 되니까 정제 작업을 거친다.
정제 작업은 총 5개로 URL 필터링, 중복 제거, 개인정보 제거, 언어 필터링, HTML 파싱으로 나뉘어져 있다.
각각 더 찾아본 결과
  - (1) URL 필터링: 스팸/성인물/낮은 품질 문서 제거
  - (2) 중복 제거: 복사 붙여넣기된 문서들을 제거
  - (3)개인정보 제거: 이메일, 전화번호 같은 민감 정보 삭제
  - (4) 언어 필터링: 어떤 언어인지 분류한 후 비율 조정 (예: 영어 위주, 한국어 일부 등)
  - (5) HTML 파싱: 웹페이지에서 실제 ‘본문 텍스트’만 추출
이런식이라고 한다.
> 이 과정을 통해 약 40TB 수준의 순수 텍스트 데이터를 확보하게 된다.

---

2) 토큰화(Tokenization) – 언어를 기계가 처리할 수 있는 조각으로ㅈ 쪼개는 방법

텍스트는 사람이 읽기엔 편하지만, 모델 입장에선 숫자가 필요하다. 그래서 텍스트를 수치화된 토큰으로 바꾸는 작업이 필요하다.

대표적인 방식으로는 BPE(Byte Pair Encoding) 방식을 사용한다! 구체적으로 보면
  - 가장 자주 등장하는 글자쌍/단어쌍을 반복적으로 묶어 새로운 단위를 만들어내는 방식이고,
  - 긴 단어도 효율적으로 표현 가능하면서, 의미도 잘 유지된다.
  - 또한 토큰 수를 줄이고, 시퀀스 길이도 압축 가능하다.

> GPT-4의 경우 약 **100,000개의 토큰 사전(vocabulary)**를 사용한다고 알려져 있다.



3) 신경망 학습 – 다음 토큰을 맞히기 위해 수천억 개의 파라미터를 조정

모델의 입력은 토큰 시퀀스고, 출력은 다음에 올 토큰을 예측한 결과다.

- 학습은 다음과 같이 진행된다.  
  > “이전 단어들을 보고 다음 단어를 맞혀봐” <- 라는 질문을 던지고,
- 이 예측이 얼마나 틀렸는지를 측정하는 게 Loss고,  
  이 값을 최소화하면서 모델의 파라미터를 조금씩 업데이트한다.
- 초기 파라미터는 랜덤 값에서 시작해서 수천억 번의 연산을 통해 언어 패턴을 익히게 된다.



4) 추론 – 학습된 확률 분포를 기반으로 새로운 문장을 생성

- 사전 학습이 끝난 모델은 그동안 봤던 데이터를 바탕으로 다음에 올 토큰의 **확률 분포**를 생성하는데,
- 항상 같은 결과를 내지는 않으며, 확률적으로 생성되기 때문에 같은 질문에 다른 답을 할 수도 있다!
- 훈련 데이터에서 봤던 내용을 그대로 재현할 수도 있지만, 대부분은 조합해서 추론하는 방식을 가진다.



5)  Base Model – 튜닝 없이, 순수하게 통계적 예측만 하는 모델

> 사전 학습만 끝난 모델은 말 그대로 '인터넷 통계 기계'에 가깝다!

즉, 사람처럼 친절하거나 질문을 이해하는 척하지 않고, 단지 과거 데이터에서 이 패턴 다음에 이런 단어가 나왔더라~를 따른다.
그래서 정확하지 않을 가능성도 존재하며 반복적으로 물으면 매번 다른 답이 도출될 수도 있다.



---
6) 사전 학습 구조

사전 학습 모델의 기억 구조를 정리해보면

파라미터는 모델이 훈련을 통해 저장한 '지식'이며, 컨텍스트 윈도우는 현재 입력으로 들어온 문장의 범위다!
예시로, GPT 모델이 2024년까지만 훈련되었더라도, 2025년 사건을 입력으로 주면 그것을 바탕으로 답변 가능하다.
> 이유는 그 정보가 파라미터 안에 있지는 않지만, 컨텍스트 안에 들어왔기 때문에 참조 가능한 것이기 때문이다!

