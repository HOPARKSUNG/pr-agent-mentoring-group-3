# 1주차 실습 과제

> 실습 과제: 개인별로 학습한 내용 정리해서 weekly/group-x/개인별이름.md 파일로 작성후 PR 보낼것

## 1. LLM Application 관련 지식 요약 리뷰 (Recap)

- 대형 언어 모델(LLM)의 작동 원리에 대해 요약 리뷰
- Meta의 환각 해결 방식과 RLHF에 집중

### Meta의 환각 해결 방식, "모름"

- Meta는 LLM이 사실이 아닌 정보를 자신 있게 생성하는 문제(환각)를 줄이기 위해, 모델이 확실하지 않을 경우 "모름(I don't know)"이라고 답변할 수 있는 기능을 실험적으로 도입.
- 무조건적인 출력보다 신뢰도를 높이는 데 효과적

### RLHF; Reinforcement Learning from Human Feedback

- RLHF는 모델의 응답을 사람이 평가하고 이를 바탕으로 모델을 보상/처벌하는 방식으로 조정하는 기술. 사전 학습된 언어 모델에 '사람 선호도'를 반영하는 fine-tuning 기법.
- 비판:

  - 1. 인간 피드백이 편향될 수 있고, 일관성 없는 기준으로 인해 모델이 지나치게 보수적이거나 모호한 응답을 하게 만들 수 있다는 비판이 있음.
  - 2. 복잡한 목적 함수를 명확히 정의하기 어렵다는 구조적 한계도 지적됨

- 참고 영상: [https://www.youtube.com/watch?v=6PTCwRRUHjE&ab_channel=Robin](https://www.youtube.com/watch?v=6PTCwRRUHjE&ab_channel=Robin)

## 2. PR-Agent 실행

- PR-Agent를 로컬 환경에 설치하고 GitHub 토큰 및 설정 파일 구성함

## 3. PR-Agent docs 렌더러 실행

> 배경: 문서 내 일부 broken 링크 발견 → 수정하고자 렌더러 실행

- 공식 문서를 로컬에서 렌더링하기 위해 `mkdocs serve` 명령어 실행
- `cairosvg`가 내부적으로 사용하는 `libcairo` 라이브러리 누락으로 실행 실패
- macOS 가상환경 문제로 식별, 환경 변수 주입하여 해결
  - `export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib`
