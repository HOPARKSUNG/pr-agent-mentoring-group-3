# 1주차 과제 - 개인 학습 내용 정리

## Git 활용(GitHub Flow 중심), Pull Request 작성 및 리뷰 방법

- GitHub Flow: GitHub에서 제안한 간단하고 효율적인 브랜치 기반의 개발 프로세스입니다
  - 보통 오픈소스 프로젝트나 빠르게 배포가 필요한 서비스 개발에서 많이 사용.
  - 전통적인 Git Flow보다 훨씬 단순화되어 있어서, **지속적인 배포(Continuous Deployment)** 에 적합.

- GitHub Flow의 주된 내용

1. main 브랜치는 언제나 배포 가능한 상태로 유지, 직접 커밋하지 않음.
2. 새로운 작업은 브랜치를 새로 만들어 시작
  - 기능 개발이든 버그 수정이든 항상 새 브랜치를 파서 작업.
  - 예: feature/add-login, bugfix/fix-typo
3. 브랜치에서 작업이 완료되면 Pull Request(PR) 생성
4. 작업을 끝냈으면 main으로 머지하기 전에 PR을 열어 코드 리뷰 요청.
5. Pull Request에서 코드 리뷰 및 토론
6. 코드 리뷰 후 main에 머지
7. main에 머지되면 자동 배포(CD)

- 예시 흐름

```
1. git checkout -b feature/add-dark-mode
2. 작업 작업 작업...
3. git push origin feature/add-dark-mode
4. GitHub에서 Pull Request 생성
5. 코드 리뷰 및 테스트 통과
6. main 브랜치에 머지
7. 자동 배포
```

- Pull Request 작성 및 리뷰 방법
  - Pull Request는 코드 변경 사항을 다른 개발자와 공유하고, 리뷰를 요청하는 방법입.
  - PR 작성 시에는 다음과 같은 내용을 포함하는 편:
    - **제목**: 간결하고 명확하게 변경 사항을 설명
    - **설명**: 변경된 내용, 이유, 관련 이슈 번호 등을 상세히 기술
    - **테스트 결과**: 변경 사항이 잘 작동하는지 확인한 결과를 포함
    - **리뷰 요청**: 특정 팀원에게 리뷰를 요청할 수 있음

## 사전 학습(LLM, Gemini API 사용법) 주제 학습
### [(한글자막) 대형 언어 모델(LLM)의 심층 분석: ChatGPT의 작동 방식 이해하기](https://www.youtube.com/watch?v=6PTCwRRUHjE)
#### 1. **대규모 언어 모델(LLM) 학습 과정**
   - **사전 학습 (Pre-training)**: 인터넷에서 수집된 방대한 텍스트 데이터로 모델을 훈련시킴. 주요 예시로는 **Findweb**(44TB 텍스트 데이터)와 **Commoncrawl**(웹 페이지 저장) 등이 있으며, 이를 통해 모델은 텍스트의 패턴을 학습하고, 특정 언어에서 유리하게 작동할 수 있음.
   - **토큰화 (Tokenization)**: 텍스트를 신경망이 이해할 수 있는 형태로 변환하는 과정. 예를 들어, UTF-8로 저장된 텍스트를 비트 단위로 변환하고, 바이트 페어 인코딩(BPE)을 사용해 반복되는 패턴을 기호화. GPT-4의 어휘 크기는 약 10만 개.
   - **신경망 학습 (Neural Network Training)**: 신경망은 예측 결과가 훈련 데이터의 패턴과 일치하도록 매개변수를 조정해 나감.
   - **추론 (Inference)**: 모델은 학습된 패턴을 바탕으로 새로운 데이터를 생성하며, 확률적으로 최적의 토큰을 선택함.

#### 2. **대화형 모델**
   - **대화 패턴 학습**: 인간 레이블러가 대화 맥락을 제공하고, 이를 바탕으로 모델이 인간과 비슷한 방식으로 대화할 수 있도록 학습시킴. 모델은 텍스트가 아닌, 대화의 흐름을 이해하고 예측할 수 있도록 훈련됨.
   - **대화의 토큰화**: 대화 중 각 사용자의 차례와 맥락을 인식하는 특수 토큰을 사용하여 대화의 흐름을 모델이 잘 이해하도록 함.

#### 3. **환각 현상 (Hallucination)**
   - **환각**은 모델이 사실과 다른 정보를 자신감 있게 생성하는 현상. 이는 모델이 훈련 데이터를 기반으로 통계적 패턴을 학습하기 때문에 발생함.
   - 이를 해결하기 위한 방법으로는 "모르겠다"라고 답하거나 외부 도구를 사용하여 확인하는 방법이 있음.

#### 4. **강화학습 (Reinforcement Learning)**
   - **강화학습**은 모델이 다양한 방법을 시도하고, 그 결과를 통해 최적의 해결책을 찾는 과정입니다. 이를 통해 모델은 시도와 오류를 반복하며 문제를 해결하는 능력을 키움.
   - **강화학습과 인간 피드백**(RLHF): 인간의 평가를 모방한 보상 모델을 통해 모델을 개선할 수 있음. 예를 들어, 농담의 순위를 평가하여 모델이 점차적으로 개선되도록 훈련.

#### 5. **모델의 한계와 가능성**
   - 모델은 다양한 분야에서 강력한 성능을 발휘하지만, **스위스 치즈 모델**처럼 특정 부분에서 오류를 발생시킬 수 있음.
   - 모델의 미래 가능성은 강화학습을 통해 더욱 향상될 수 있으며, 비정형적 작업이나 창작 활동에서 좋은 성과를 낼 수 있음.

#### 6. **미래의 AI 모델**
   - **사전학습과 후속 훈련**: 사전학습은 기본적인 텍스트 예측 능력을 제공하고, 후속 훈련을 통해 모델은 특정 작업에 최적화됨.
   - **인공지능의 인간처럼 대화하는 능력**은 후속 훈련과 대화형 데이터셋을 통해 가능하며, 더 나아가 **자기 인식**을 하는 듯한 대화 방식도 도입될 수 있음.

## Gemini API
* [링크](https://ai.google.dev/gemini-api/docs/get-started/tutorial?hl=ko&lang=web) 참고하였음.
* 과거 OpenAI API를 활용해 날씨 챗봇을 만들어 본 적이 있는데, 그때와 유사한 방식으로 사용 가능했음. (React, TypeScript 기반 웹앱)

## 회고
* Git 기본 개념과 활용과 익숙한 편이나, LLM 관련 지식은 비교적 생소하여 습득이 어려웠음. 
* Gemini API 사용법은 문서화가 잘 되어 있어, API를 활용하는 데 큰 어려움은 없었음.
* 돌아오는 주엔 학습한 내용을 바탕으로 PR-Agent에 관해 좀 더 깊이 있는 학습으로 이어가고자 함.
